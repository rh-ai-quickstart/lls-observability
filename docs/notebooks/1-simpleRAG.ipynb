{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# ğŸ¦™ LlamaStack & RAG: Building Intelligent Agents\n",
    "\n",
    "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** - a powerful technique that enables AI models to access and reason about external documents and knowledge bases.\n",
    "\n",
    "**What is RAG?**\n",
    "RAG transforms static AI models into dynamic assistants that can:\n",
    "- **Remember** every document you share with them\n",
    "- **Search** through vast libraries of content in milliseconds  \n",
    "- **Reason** about information from multiple sources simultaneously\n",
    "- **Update** their knowledge without retraining the entire model\n",
    "\n",
    "**Why RAG Matters:**\n",
    "Instead of relying only on training data, RAG-enhanced models can reference your specific documents, course materials, and knowledge bases to provide accurate, cited responses.\n",
    "\n",
    "You've already built a vector database - now let's add the intelligent layer that makes RAG truly powerful! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ The LlamaStack RAG Architecture\n",
    "\n",
    "LlamaStack organizes RAG capabilities into **three elegant layers** that work together to create intelligent, knowledge-aware applications:\n",
    "\n",
    "### 1. ğŸ—„ï¸ Storage Layer (The Foundation)\n",
    "This is where your knowledge lives:\n",
    "- **Vector IO**: Stores document embeddings for semantic search - converts text into mathematical vectors that capture meaning\n",
    "- **KeyValue IO**: Manages structured metadata and simple lookups (document titles, authors, dates)\n",
    "- **Relational IO**: Handles complex queries across structured data (coming soon)\n",
    "\n",
    "### 2. ğŸ”§ RAG Layer (The Intelligence)\n",
    "This is where documents become searchable knowledge:\n",
    "- **Document Ingestion**: Automatically downloads and processes files, URLs, and content\n",
    "- **Intelligent Chunking**: Splits large documents into optimal pieces (typically 512 tokens) for retrieval\n",
    "- **Semantic Search**: Finds relevant content based on meaning, not just keyword matching\n",
    "\n",
    "### 3. ğŸ¤– User Layer (The Interface)  \n",
    "This is where users interact with the knowledge:\n",
    "- **Context-Aware Agents**: LLMs that can automatically use RAG tools to answer questions\n",
    "- **Multi-Document Reasoning**: Agents that synthesize information from multiple sources\n",
    "- **Conversational Memory**: Maintains context across interactions while accessing external knowledge\n",
    "\n",
    "**The Magic:** When you ask a question, the system searches the vector database for relevant chunks, then provides those chunks as context to the LLM for generating informed, cited responses.\n",
    "\n",
    "## ğŸ“¦ Install Required Packages\n",
    "\n",
    "Install the Python packages needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama_stack_client fire dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core imports for RAG functionality\n",
    "import uuid  # For generating unique vector database IDs\n",
    "\n",
    "# LlamaStack client and RAG-specific classes\n",
    "from llama_stack_client import RAGDocument  # Represents documents for ingestion\n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta  # For streaming responses\n",
    "\n",
    "# Additional utilities for document processing\n",
    "import base64    # For encoding images/binary data if needed\n",
    "import requests  # For fetching documents from URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "## ğŸ”— Connect to LlamaStack\n",
    "\n",
    "Connect to LlamaStack - the AI engine that orchestrates all RAG operations. LlamaStack acts as the central hub that coordinates:\n",
    "- Vector database operations (storage and retrieval)\n",
    "- Document processing and chunking\n",
    "- LLM inference with retrieved context\n",
    "- Agent workflows and tool usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LlamaStack server at http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321\n",
      "Model: llama32\n",
      "Sampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 512}\n",
      "Stream: False\n"
     ]
    }
   ],
   "source": [
    "# Standard imports for system utilities\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import LlamaStack client and utilities\n",
    "from termcolor import cprint        # For colorized console output\n",
    "from llama_stack_client import LlamaStackClient  # Main client for all LlamaStack operations\n",
    "\n",
    "# === LlamaStack Connection Setup ===\n",
    "# The base URL points to your LlamaStack server deployment\n",
    "base_url = \"http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321\"\n",
    "\n",
    "# Optional: Configure external search tools (Tavily for web search)\n",
    "# Leave empty for this RAG-focused demo\n",
    "tavily_search_api_key = \"\"\n",
    "if tavily_search_api_key:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "else:\n",
    "    provider_data = None\n",
    "\n",
    "# Create the LlamaStack client - this is your main interface for all RAG operations\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data  # Additional provider configurations\n",
    ")\n",
    "\n",
    "print(f\"Connected to LlamaStack server at {base_url}\")\n",
    "\n",
    "# === Model Configuration ===\n",
    "# Specify which LLM model to use for generating responses\n",
    "model_id = \"llama32\"  # Using Llama 3.2 model name\n",
    "\n",
    "# === Generation Parameters ===\n",
    "# These control how the model generates responses\n",
    "temperature = 0.0  # 0.0 = deterministic, higher = more creative\n",
    "max_tokens = 512   # Maximum length of generated responses\n",
    "stream = False     # Whether to stream responses token-by-token\n",
    "\n",
    "# Configure the sampling strategy based on temperature\n",
    "if temperature > 0.0:\n",
    "    top_p = 0.95  # Nucleus sampling parameter\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}  # Always pick most likely token\n",
    "\n",
    "# Package sampling parameters for the inference API\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "# Display configuration for verification\n",
    "print(f\"Model: {model_id}\")\n",
    "print(f\"Sampling Parameters: {sampling_params}\")\n",
    "print(f\"Stream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "## ğŸ—ƒï¸ Create Vector Database for RAG\n",
    "\n",
    "Set up a vector database where documents will be stored for retrieval. This is the **Storage Layer** of our RAG architecture.\n",
    "\n",
    "**What happens here:**\n",
    "1. **Registration**: Tell LlamaStack about your vector database configuration\n",
    "2. **Embedding Model**: Specify which model converts text to vectors (we use `all-MiniLM-L6-v2`)\n",
    "3. **Dimensions**: Set vector size (384 dimensions for our chosen model)\n",
    "4. **Provider**: Connect to your Milvus database deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Created vector database ID: test_vector_db_59bd4905-c23e-4d07-9c00-1cc05a2e59e2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/vector-dbs \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Registered vector database with Milvus backend\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: Register Vector Database ===\n",
    "\n",
    "# Generate a unique identifier for this vector database instance\n",
    "# Using UUID ensures no conflicts if multiple users run this notebook simultaneously\n",
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\"\n",
    "print(f\"ğŸ“Š Created vector database ID: {vector_db_id}\")\n",
    "\n",
    "# This tells LlamaStack how to connect to and use your vector database\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,                    # Unique identifier we created above\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",          # Hugging Face model for text â†’ vectors\n",
    "    embedding_dimension=384,                      # Vector size (must match model output)\n",
    "    provider_id=\"milvus\",                         # Use Milvus as the vector database backend\n",
    ")\n",
    "print(f\"âœ… Registered vector database with Milvus backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## ğŸ“š Document Ingestion and Processing\n",
    "\n",
    "This is where the **RAG Layer** comes into action! We'll use LlamaStack's RAG Tool to automatically:\n",
    "\n",
    "1. **Download** documents from URLs\n",
    "2. **Process** PDF content and extract text\n",
    "3. **Chunk** large documents into optimal pieces (512 tokens each)\n",
    "4. **Embed** each chunk using the embedding model\n",
    "5. **Store** vectors and metadata in the vector database\n",
    "\n",
    "**Two ways to ingest documents:**\n",
    "- **Direct Vector IO**: Insert pre-processed chunks directly\n",
    "- **RAG Tool** (what we're using): Automatic processing from URLs or files\n",
    "\n",
    "A better way to ingest documents is to use the RAG Tool. This tool allows you to ingest documents from URLs, files, etc. and automatically chunks them into smaller pieces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Ingesting documents into RAG system...\n",
      "  â€¢ Document 1: https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/tool-runtime/rag-tool/insert \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Document ingestion complete!\n",
      "ğŸ¯ Your documents are now searchable via semantic similarity!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: Define Documents to Ingest ===\n",
    "# List of (URL, MIME_TYPE) tuples for documents to process\n",
    "urls = [\n",
    "    (\"https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf\", \"application/pdf\"),\n",
    "]\n",
    "\n",
    "# === STEP 3: Create RAGDocument Objects ===\n",
    "# RAGDocument is LlamaStack's format for documents to be ingested\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"doc-{i}\",                   # Unique ID for this document\n",
    "        content=url,                              # Can be URL, file path, or direct text\n",
    "        mime_type=url_type,                       # Tells LlamaStack how to process the content\n",
    "        metadata={                                # Additional information about the document\n",
    "            \"source_url\": url,                    # Where this document came from\n",
    "            \"document_type\": \"academic_material\",  # Category for filtering/organization\n",
    "        },\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "\n",
    "# Display what we're about to ingest\n",
    "print(\"ğŸ“– Ingesting documents into RAG system...\")\n",
    "for i, (url, url_type) in enumerate(urls):\n",
    "    print(f\"  â€¢ Document {i+1}: {url}\")\n",
    "\n",
    "# === STEP 4: Use RAG Tool for Automatic Processing ===\n",
    "# This is where the magic happens! The RAG tool will:\n",
    "# 1. Download the PDF from the URL\n",
    "# 2. Extract and parse the text content\n",
    "# 3. Split into chunks of 512 tokens each\n",
    "# 4. Generate embeddings for each chunk\n",
    "# 5. Store everything in the vector database\n",
    "try:\n",
    "    client.tool_runtime.rag_tool.insert(\n",
    "        documents=documents,                      # List of RAGDocument objects to process\n",
    "        vector_db_id=vector_db_id,               # Where to store the processed chunks\n",
    "        chunk_size_in_tokens=512,                # Optimal size for retrieval (not too big, not too small)\n",
    "    )\n",
    "    print(\"\\nâœ… Document ingestion complete!\")\n",
    "    print(\"ğŸ¯ Your documents are now searchable via semantic similarity!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Document ingestion failed: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to PDF processing issues. Try with different documents or check the PDF accessibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## ğŸ” Testing RAG Retrieval and Generation\n",
    "\n",
    "Now let's test the complete **RAG Pipeline** - this demonstrates how all three layers work together:\n",
    "\n",
    "### The RAG Process:\n",
    "1. **ğŸ” Query Processing**: Convert user question into embeddings\n",
    "2. **ğŸ“š Semantic Retrieval**: Find most similar document chunks in vector database  \n",
    "3. **ğŸ”— Context Assembly**: Combine user question with retrieved chunks\n",
    "4. **ğŸ¤– Generation**: LLM generates informed response using both its training and the retrieved context\n",
    "5. **ğŸ“– Citation**: Response includes references to source documents\n",
    "\n",
    "**Why this works better than normal LLMs:**\n",
    "- **Grounded responses**: Answers are based on your specific documents\n",
    "- **Up-to-date**: Add new documents without retraining the model\n",
    "- **Traceable**: Every answer can be traced back to source material\n",
    "- **Accurate**: Reduces hallucination by providing factual context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/tool-runtime/rag-tool/query \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/inference/chat-completion \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> What is the revenue of ParaCloud Corp in FY2025?\u001b[0m\n",
      "QueryResult(metadata={'document_ids': ['doc-0', 'doc-0', 'doc-0'], 'chunks': ['Parasol Cloud Corp â€“ FY2025 Earnings Report\\nExecutive Summary\\nParasol Cloud Corp delivered strong performance in FY2025, achieving record revenue\\ngrowth driven by subscription renewals and strategic upsells. The company maintained\\na healthy opportunity pipeline, with active opportunities contributing to a projected\\n18% YoY growth. Customer engagement remains strong, with significant\\nimprovements in SLA compliance despite an increase in high-severity support cases.\\n\\nFinancial Highlights\\nTotal recognized revenue for FY2025 reached $48M, with $25M from enterprise\\nlicense renewals and $15M from subscription renewals. Cloud package upsells\\ncontributed $5M, representing a 20% increase from last year. The closed opportunities\\nfrom legacy support contributed $8M but declined compared to FY2024 due to\\nproduct sunset initiatives.\\nRevenue Stream\\nFY2024 ($M)\\nFY2025 ($M)\\nYoY Growth\\nSubscription Renewals\\n12.5\\n15.0\\n20%\\nEnterprise License\\n22.0\\n25.0\\n14%\\nUpsells & Expansion\\n4.2\\n5.0\\n19%\\nLegacy Support\\n10.0\\n8.0\\n-20%\\nTotal\\n48.7\\n53.0\\n9%\\n\\nCustomer & Account Insights\\nAcme Corp and Globex Inc remain top contributors to revenue, together representing\\n65% of total billings. Acme Corp showed strong adoption of new cloud packages, while\\nGlobex Inc continues to rely heavily on enterprise licensing. Soylent Corp, while\\nrepresenting a smaller share, maintains a consistent revenue stream from legacy\\nproducts. Customer churn remains below 5%, an industry-leading figure.\\n\\nOpportunity Pipeline Analysis\\nActive opportunities account for $45M in potential revenue. Tier A renewals have\\nshown particularly strong performance, with 90% renewal rates projected. The pipeline\\nhealth index remains at 0.82, indicating a well-balanced mix of early, mid, and\\nlate-stage deals. The closed opportunities show a natural decline due to the phase-out\\nof legacy contracts, aligning with corporate strategy.\\n\\nSupport Case Analysis\\nA total of 6 support cases were logged during this reporting period. 50% of cases were\\nclassified as high or critical severity, driven by API reliability issues and dashboard\\nperformance incidents. Median resolution time improved by 12% compared to FY2024,\\nhighlighting operational efficiencies in the support organization.\\n\\nYear-over-Year Comparisons\\nRevenue grew 9% Yo', 'through joint innovation workshops.\\n\\nAppendix: Raw Data Summary\\nOpportunities: - (1) Active | Account: Acme Corp | Items: $15,000 (Tier A), $5,000\\n(Cloud Upsell) - (2) Active | Account: Globex Inc | Items: $25,000 (Enterprise License\\nRenewal) - (3) Closed | Account: Soylent Corp | Items: $8,000 (Legacy Support)\\nSupport Cases: - High severity login and dashboard issues logged for Acme Corp. -\\nMedium payment issue and high-severity email delivery issue for Globex Inc. - Critical\\nAPI outage for Soylent Corp. Resolution rates and severity breakdown indicate a focus\\narea for reliability improvements.\\n', '\\nhealth index remains at 0.82, indicating a well-balanced mix of early, mid, and\\nlate-stage deals. The closed opportunities show a natural decline due to the phase-out\\nof legacy contracts, aligning with corporate strategy.\\n\\nSupport Case Analysis\\nA total of 6 support cases were logged during this reporting period. 50% of cases were\\nclassified as high or critical severity, driven by API reliability issues and dashboard\\nperformance incidents. Median resolution time improved by 12% compared to FY2024,\\nhighlighting operational efficiencies in the support organization.\\n\\nYear-over-Year Comparisons\\nRevenue grew 9% YoY, while operating costs rose by 6%, resulting in improved\\noperating margins. Customer satisfaction scores increased from 86 to 89, driven by\\nfaster response times and product stability. Churn rates decreased by 1.5 points\\ncompared to FY2024, largely attributed to proactive engagement programs.\\n\\nOperational Metrics\\nSLA compliance improved to 98.5%, with major incidents handled within contractual\\nwindows. First-contact resolution rates increased to 72%, up from 65% last year. The\\nsupport team has invested in automation and self-service capabilities, reducing\\naverage case handling time from 4.5 hours to 3.8 hours.\\n\\nFuture Outlook\\nParasol Cloud Corp is positioned for strong growth in FY2026, with plans to expand\\ninto three new regional markets and introduce AI-powered analytics capabilities. The\\nsales team is focused on deepening relationships with existing enterprise clients while\\npursuing net-new accounts in the financial services sector.\\n\\nRecommendations\\n1. Continue to invest in API stability to reduce critical case volume. 2. Expand upsell\\ncampaigns targeting mid-sized enterprise clients. 3. Leverage automation to further\\nreduce case handling times. 4. Strengthen partnerships with strategic accounts\\nthrough joint innovation workshops.\\n\\nAppendix: Raw Data Summary\\nOpportunities: - (1) Active | Account: Acme Corp | Items: $15,000 (Tier A), $5,000\\n(Cloud Upsell) - (2) Active | Account: Globex Inc | Items: $25,000 (Enterprise License\\nRenewal) - (3) Closed | Account: Soylent Corp | Items: $8,000 (Legacy Support)\\nSupport Cases: - High severity login and dashboard issues logged for Acme Corp. -\\nMedium payment issue and high-severity email delivery issue for Globex Inc'], 'scores': [0.6499752402305603, 0.43983548879623413, 0.39666417241096497]}, content=[TextContentItem(text='knowledge_search tool found 3 chunks:\\nBEGIN of knowledge_search tool results.\\n', type='text'), TextContentItem(text=\"Result 1\\nContent: Parasol Cloud Corp â€“ FY2025 Earnings Report\\nExecutive Summary\\nParasol Cloud Corp delivered strong performance in FY2025, achieving record revenue\\ngrowth driven by subscription renewals and strategic upsells. The company maintained\\na healthy opportunity pipeline, with active opportunities contributing to a projected\\n18% YoY growth. Customer engagement remains strong, with significant\\nimprovements in SLA compliance despite an increase in high-severity support cases.\\n\\nFinancial Highlights\\nTotal recognized revenue for FY2025 reached $48M, with $25M from enterprise\\nlicense renewals and $15M from subscription renewals. Cloud package upsells\\ncontributed $5M, representing a 20% increase from last year. The closed opportunities\\nfrom legacy support contributed $8M but declined compared to FY2024 due to\\nproduct sunset initiatives.\\nRevenue Stream\\nFY2024 ($M)\\nFY2025 ($M)\\nYoY Growth\\nSubscription Renewals\\n12.5\\n15.0\\n20%\\nEnterprise License\\n22.0\\n25.0\\n14%\\nUpsells & Expansion\\n4.2\\n5.0\\n19%\\nLegacy Support\\n10.0\\n8.0\\n-20%\\nTotal\\n48.7\\n53.0\\n9%\\n\\nCustomer & Account Insights\\nAcme Corp and Globex Inc remain top contributors to revenue, together representing\\n65% of total billings. Acme Corp showed strong adoption of new cloud packages, while\\nGlobex Inc continues to rely heavily on enterprise licensing. Soylent Corp, while\\nrepresenting a smaller share, maintains a consistent revenue stream from legacy\\nproducts. Customer churn remains below 5%, an industry-leading figure.\\n\\nOpportunity Pipeline Analysis\\nActive opportunities account for $45M in potential revenue. Tier A renewals have\\nshown particularly strong performance, with 90% renewal rates projected. The pipeline\\nhealth index remains at 0.82, indicating a well-balanced mix of early, mid, and\\nlate-stage deals. The closed opportunities show a natural decline due to the phase-out\\nof legacy contracts, aligning with corporate strategy.\\n\\nSupport Case Analysis\\nA total of 6 support cases were logged during this reporting period. 50% of cases were\\nclassified as high or critical severity, driven by API reliability issues and dashboard\\nperformance incidents. Median resolution time improved by 12% compared to FY2024,\\nhighlighting operational efficiencies in the support organization.\\n\\nYear-over-Year Comparisons\\nRevenue grew 9% Yo\\nMetadata: {'source_url': 'https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf', 'document_type': 'academic_material', 'document_id': 'doc-0'}\\n\", type='text'), TextContentItem(text=\"Result 2\\nContent: through joint innovation workshops.\\n\\nAppendix: Raw Data Summary\\nOpportunities: - (1) Active | Account: Acme Corp | Items: $15,000 (Tier A), $5,000\\n(Cloud Upsell) - (2) Active | Account: Globex Inc | Items: $25,000 (Enterprise License\\nRenewal) - (3) Closed | Account: Soylent Corp | Items: $8,000 (Legacy Support)\\nSupport Cases: - High severity login and dashboard issues logged for Acme Corp. -\\nMedium payment issue and high-severity email delivery issue for Globex Inc. - Critical\\nAPI outage for Soylent Corp. Resolution rates and severity breakdown indicate a focus\\narea for reliability improvements.\\n\\nMetadata: {'source_url': 'https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf', 'document_type': 'academic_material', 'document_id': 'doc-0'}\\n\", type='text'), TextContentItem(text=\"Result 3\\nContent: \\nhealth index remains at 0.82, indicating a well-balanced mix of early, mid, and\\nlate-stage deals. The closed opportunities show a natural decline due to the phase-out\\nof legacy contracts, aligning with corporate strategy.\\n\\nSupport Case Analysis\\nA total of 6 support cases were logged during this reporting period. 50% of cases were\\nclassified as high or critical severity, driven by API reliability issues and dashboard\\nperformance incidents. Median resolution time improved by 12% compared to FY2024,\\nhighlighting operational efficiencies in the support organization.\\n\\nYear-over-Year Comparisons\\nRevenue grew 9% YoY, while operating costs rose by 6%, resulting in improved\\noperating margins. Customer satisfaction scores increased from 86 to 89, driven by\\nfaster response times and product stability. Churn rates decreased by 1.5 points\\ncompared to FY2024, largely attributed to proactive engagement programs.\\n\\nOperational Metrics\\nSLA compliance improved to 98.5%, with major incidents handled within contractual\\nwindows. First-contact resolution rates increased to 72%, up from 65% last year. The\\nsupport team has invested in automation and self-service capabilities, reducing\\naverage case handling time from 4.5 hours to 3.8 hours.\\n\\nFuture Outlook\\nParasol Cloud Corp is positioned for strong growth in FY2026, with plans to expand\\ninto three new regional markets and introduce AI-powered analytics capabilities. The\\nsales team is focused on deepening relationships with existing enterprise clients while\\npursuing net-new accounts in the financial services sector.\\n\\nRecommendations\\n1. Continue to invest in API stability to reduce critical case volume. 2. Expand upsell\\ncampaigns targeting mid-sized enterprise clients. 3. Leverage automation to further\\nreduce case handling times. 4. Strengthen partnerships with strategic accounts\\nthrough joint innovation workshops.\\n\\nAppendix: Raw Data Summary\\nOpportunities: - (1) Active | Account: Acme Corp | Items: $15,000 (Tier A), $5,000\\n(Cloud Upsell) - (2) Active | Account: Globex Inc | Items: $25,000 (Enterprise License\\nRenewal) - (3) Closed | Account: Soylent Corp | Items: $8,000 (Legacy Support)\\nSupport Cases: - High severity login and dashboard issues logged for Acme Corp. -\\nMedium payment issue and high-severity email delivery issue for Globex Inc\\nMetadata: {'source_url': 'https://raw.githubusercontent.com/rh-ai-quickstart/lls-observability/dev/assets/images/Parasol_Cloud_Corp_Earnings_Report.pdf', 'document_type': 'academic_material', 'document_id': 'doc-0'}\\n\", type='text'), TextContentItem(text='END of knowledge_search tool results.\\n', type='text'), TextContentItem(text='The above results were retrieved to help answer the user\\'s query: \"What is the revenue of ParaCloud Corp in FY2025?\". Use them as supporting information only in answering this query.\\n', type='text')])\u001b[0m\n",
      "\u001b[33m\n",
      "--- RAG Metadata ---\u001b[0m\n",
      "\u001b[36m{'document_ids': ['doc-0', 'doc-0', 'doc-0'], 'chunks': ['Parasol Cloud Corp â€“ FY2025 Earnings Report\\nExecutive Summary\\nParasol Cloud Corp delivered strong performance in FY2025, achieving record revenue\\ngrowth driven by subscription renewals and strategic upsells. The company maintained\\na healthy opportunity pipeline, with active opportunities contributing to a projected\\n18% YoY growth. Customer engagement remains strong, with significant\\nimprovements in SLA compliance despite an increase in high-severity support cases.\\n\\nFinancial Highlights\\nTotal recognized revenue for FY2025 reached $48M, with $25M from enterprise\\nlicense renewals and $15M from subscription renewals. Cloud package upsells\\ncontributed $5M, representing a 20% increase from last year. The closed opportunities\\nfrom legacy support contributed $8M but declined compared to FY2024 due to\\nproduct sunset initiatives.\\nRevenue Stream\\nFY2024 ($M)\\nFY2025 ($M)\\nYoY Growth\\nSubscription Renewals\\n12.5\\n15.0\\n20%\\nEnterprise License\\n22.0\\n25.0\\n14%\\nUpsells & Expansion\\n4.2\\n5.0\\n19%\\nLegacy Support\\n10.0\\n8.0\\n-20%\\nTotal\\n48.7\\n53.0\\n9%\\n\\nCustomer & Account Insights\\nAcme Corp and Globex Inc remain top contributors to revenue, together representing\\n65% of total billings. Acme Corp showed strong adoption of new cloud packages, while\\nGlobex Inc continues to rely heavily on enterprise licensing. Soylent Corp, while\\nrepresenting a smaller share, maintains a consistent revenue stream from legacy\\nproducts. Customer churn remains below 5%, an industry-leading figure.\\n\\nOpportunity Pipeline Analysis\\nActive opportunities account for $45M in potential revenue. Tier A renewals have\\nshown particularly strong performance, with 90% renewal rates projected. The pipeline\\nhealth index remains at 0.82, indicating a well-balanced mix of early, mid, and\\nlate-stage deals. The closed opportunities show a natural decline due to the phase-out\\nof legacy contracts, aligning with corporate strategy.\\n\\nSupport Case Analysis\\nA total of 6 support cases were logged during this reporting period. 50% of cases were\\nclassified as high or critical severity, driven by API reliability issues and dashboard\\nperformance incidents. Median resolution time improved by 12% compared to FY2024,\\nhighlighting operational efficiencies in the support organization.\\n\\nYear-over-Year Comparisons\\nRevenue grew 9% Yo', 'through joint innovation workshops.\\n\\nAppendix: Raw Data Summary\\nOpportunities: - (1) Active | Account: Acme Corp | Items: $15,000 (Tier A), $5,000\\n(Cloud Upsell) - (2) Active | Account: Globex Inc | Items: $25,000 (Enterprise License\\nRenewal) - (3) Closed | Account: Soylent Corp | Items: $8,000 (Legacy Support)\\nSupport Cases: - High severity login and dashboard issues logged for Acme Corp. -\\nMedium payment issue and high-severity email delivery issue for Globex Inc. - Critical\\nAPI outage for Soylent Corp. Resolution rates and severity breakdown indicate a focus\\narea for reliability improvements.\\n', '\\nhealth index remains at 0.82, indicating a well-balanced mix of early, mid, and\\nlate-stage deals. The closed opportunities show a natural decline due to the phase-out\\nof legacy contracts, aligning with corporate strategy.\\n\\nSupport Case Analysis\\nA total of 6 support cases were logged during this reporting period. 50% of cases were\\nclassified as high or critical severity, driven by API reliability issues and dashboard\\nperformance incidents. Median resolution time improved by 12% compared to FY2024,\\nhighlighting operational efficiencies in the support organization.\\n\\nYear-over-Year Comparisons\\nRevenue grew 9% YoY, while operating costs rose by 6%, resulting in improved\\noperating margins. Customer satisfaction scores increased from 86 to 89, driven by\\nfaster response times and product stability. Churn rates decreased by 1.5 points\\ncompared to FY2024, largely attributed to proactive engagement programs.\\n\\nOperational Metrics\\nSLA compliance improved to 98.5%, with major incidents handled within contractual\\nwindows. First-contact resolution rates increased to 72%, up from 65% last year. The\\nsupport team has invested in automation and self-service capabilities, reducing\\naverage case handling time from 4.5 hours to 3.8 hours.\\n\\nFuture Outlook\\nParasol Cloud Corp is positioned for strong growth in FY2026, with plans to expand\\ninto three new regional markets and introduce AI-powered analytics capabilities. The\\nsales team is focused on deepening relationships with existing enterprise clients while\\npursuing net-new accounts in the financial services sector.\\n\\nRecommendations\\n1. Continue to invest in API stability to reduce critical case volume. 2. Expand upsell\\ncampaigns targeting mid-sized enterprise clients. 3. Leverage automation to further\\nreduce case handling times. 4. Strengthen partnerships with strategic accounts\\nthrough joint innovation workshops.\\n\\nAppendix: Raw Data Summary\\nOpportunities: - (1) Active | Account: Acme Corp | Items: $15,000 (Tier A), $5,000\\n(Cloud Upsell) - (2) Active | Account: Globex Inc | Items: $25,000 (Enterprise License\\nRenewal) - (3) Closed | Account: Soylent Corp | Items: $8,000 (Legacy Support)\\nSupport Cases: - High severity login and dashboard issues logged for Acme Corp. -\\nMedium payment issue and high-severity email delivery issue for Globex Inc'], 'scores': [0.6499752402305603, 0.43983548879623413, 0.39666417241096497]}\u001b[0m\n",
      "\u001b[35minference> \u001b[0m\u001b[34m\n",
      "--- End of RAG Answer ---\u001b[0m\n",
      "\n",
      "ğŸ‰ RAG Pipeline Complete!\n",
      "ğŸ” Notice how the responses reference specific information from the documents\n",
      "ğŸ“š This is the power of RAG: grounded, factual, and citable answers\n"
     ]
    }
   ],
   "source": [
    "# === Test Queries ===\n",
    "# These questions will test our RAG system's ability to find and synthesize information\n",
    "queries = [\n",
    "    \"What is the revenue of ParaCloud Corp in FY2025?\",        # Tests retrieval of categorical information\n",
    "]\n",
    "\n",
    "# === RAG Pipeline Testing Loop ===\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # === STEP 1: RAG RETRIEVAL ===\n",
    "    # Query the vector database to find relevant document chunks\n",
    "    # This uses semantic similarity - the question gets converted to embeddings\n",
    "    # and matched against document chunk embeddings\n",
    "    rag_response = client.tool_runtime.rag_tool.query(\n",
    "        content=prompt,                              # The user's question\n",
    "        vector_db_ids=[vector_db_id],               # Which vector database(s) to search\n",
    "        query_config={                              # How to format the retrieved results\n",
    "            \"chunk_template\": \"Result {index}\\nContent: {chunk.content}\\nMetadata: {metadata}\\n\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Display the full RAG response structure\n",
    "    cprint(rag_response)\n",
    "\n",
    "    # === STEP 2: EXAMINE RETRIEVED METADATA ===\n",
    "    # The metadata contains information about which documents were matched\n",
    "    # and their relevance scores\n",
    "    cprint(f\"\\n--- RAG Metadata ---\", \"yellow\")\n",
    "    cprint(rag_response.metadata, \"cyan\")\n",
    "\n",
    "    # === STEP 3: PREPARE MESSAGES FOR LLM ===\n",
    "    # Structure the conversation with system prompt and user query\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "\n",
    "    # === STEP 4: CONTEXT INJECTION ===\n",
    "    # This is the key to RAG: we inject the retrieved content as context\n",
    "    # The LLM now has both its training knowledge AND the specific document content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # === STEP 5: LLM GENERATION WITH CONTEXT ===\n",
    "    # The LLM generates a response using both its training and the retrieved context\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,                          # The conversation including context\n",
    "        model_id=model_id,                         # Which model to use for generation\n",
    "        sampling_params=sampling_params,           # How to generate (greedy vs sampling)\n",
    "        stream=True,                               # Stream the response token by token\n",
    "    )\n",
    "\n",
    "    # === STEP 6: DISPLAY GENERATED RESPONSE ===\n",
    "    # Show the final answer that combines the LLM's knowledge with document facts\n",
    "    cprint(\"inference> \", color=\"magenta\", end='')\n",
    "    \n",
    "    # Handle streaming response - tokens arrive one by one\n",
    "    for event in response:\n",
    "        # Some SDKs surface \"delta\" at the top level; others nest under \".event\"\n",
    "        ev = getattr(event, \"event\", event)  # Fall back to event itself\n",
    "        delta = getattr(ev, \"delta\", None)\n",
    "\n",
    "        if delta is None:\n",
    "            # Non-delta events: message_start, message_end, tool_started, heartbeats, etc.\n",
    "            continue\n",
    "\n",
    "        # Extract and display text tokens as they arrive\n",
    "        text = getattr(delta, \"text\", None)\n",
    "        if isinstance(text, str):\n",
    "            cprint(text, color=\"magenta\", end='')\n",
    "            continue\n",
    "\n",
    "        # Handle any tool call tokens (if the model decides to use tools)\n",
    "        tool_call = getattr(delta, \"tool_call\", None)\n",
    "        if tool_call is not None:\n",
    "            cprint(str(tool_call), color=\"magenta\", end='')\n",
    "            continue\n",
    "    \n",
    "    cprint(f\"\\n--- End of RAG Answer ---\", \"blue\")\n",
    "\n",
    "print(\"\\nğŸ‰ RAG Pipeline Complete!\")\n",
    "print(\"ğŸ” Notice how the responses reference specific information from the documents\")\n",
    "print(\"ğŸ“š This is the power of RAG: grounded, factual, and citable answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": [
    "## ğŸ‰ You've Built a Complete RAG System!\n",
    "\n",
    "**What you accomplished:**\n",
    "- **ğŸ—„ï¸ Storage Layer**: Registered and configured Milvus vector database with proper embeddings\n",
    "- **ğŸ”§ RAG Layer**: Used LlamaStack's RAG Tool for automatic document processing and chunking\n",
    "- **ğŸ¤– User Layer**: Built query processing with context-aware generation and streaming responses\n",
    "- **ğŸ“Š End-to-End Pipeline**: Demonstrated retrieval â†’ context injection â†’ generation â†’ citation\n",
    "\n",
    "**Key Technical Insights:**\n",
    "- **Semantic Search**: Questions find relevant content by meaning, not just keyword matching\n",
    "- **Document Chunking**: Large documents are split optimally (512 tokens) for precise retrieval\n",
    "- **Context Injection**: The magic happens when retrieved chunks become context for the LLM\n",
    "- **Grounded Generation**: Responses are factual because they reference specific document content\n",
    "\n",
    "**RAG vs Standard LLMs:**\n",
    "| Standard LLM | RAG-Enhanced LLM |\n",
    "|--------------|------------------|\n",
    "| âŒ Limited to training data | âœ… Access to your documents |\n",
    "| âŒ Can hallucinate facts | âœ… Grounded in real sources |\n",
    "| âŒ No citations | âœ… Traceable references |\n",
    "| âŒ Static knowledge | âœ… Updatable knowledge base |\n",
    "\n",
    "**Advanced RAG Patterns to Explore:**\n",
    "- **Multi-Document Reasoning**: Synthesize information across multiple sources\n",
    "- **Conversational RAG**: Maintain context across multiple questions\n",
    "- **Hybrid Search**: Combine semantic and keyword search\n",
    "- **Agent Workflows**: Let AI agents decide when and how to search documents\n",
    "\n",
    "Your RAG system can now intelligently answer questions using document knowledge - the foundation for intelligent, knowledge-aware applications! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
