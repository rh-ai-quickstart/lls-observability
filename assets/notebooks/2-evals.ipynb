{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": "# ğŸ§ª Evaluating Llama Stack Models with Built-in Evaluation Framework\n\nThis notebook demonstrates **Model Evaluation** - a critical process for ensuring AI models perform reliably and meet quality standards before production deployment.\n\n**What is Model Evaluation?**\nModel evaluation transforms uncertainty into confidence by:\n- **Testing** models against known correct answers and expected behaviors\n- **Measuring** performance across different types of questions and tasks\n- **Identifying** potential issues and failure modes before deployment\n- **Comparing** different models or configurations systematically\n\n**Why Evaluation Matters:**\nInstead of hoping your model works correctly, evaluation provides objective metrics and insights:\n- **Quality Assurance**: Verify models meet performance thresholds\n- **Regression Testing**: Ensure changes don't break existing functionality  \n- **Model Selection**: Compare different models to choose the best one\n- **Continuous Improvement**: Track performance over time and identify areas for enhancement\n\n**Llama Stack's Evaluation Framework:**\nLlama Stack provides three core APIs for comprehensive evaluation:\n- ğŸ“Š **`/datasetio` + `/datasets`** - Managing evaluation datasets\n- ğŸ¯ **`/scoring` + `/scoring_functions`** - Running scoring functions  \n- ğŸ“ˆ **`/eval` + `/benchmarks`** - Comprehensive evaluation workflows\n\nLet's explore both strict and flexible evaluation methods to build robust quality assurance! ğŸš€"
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": "## ğŸ—ï¸ Understanding Evaluation Methods\n\nLlamaStack provides **two primary evaluation approaches** that work together to provide comprehensive model testing:\n\n### 1. ğŸ¯ Exact Matching: `subset_of`\nThis method performs strict string matching:\n- **Speed**: Lightning fast - simple substring search\n- **Precision**: Exact character-by-character matching \n- **Use Case**: Perfect for factual questions with clear, specific answers\n- **Limitation**: Case-sensitive and inflexible to paraphrasing\n\n**Example**: \"Paris\" âœ… matches \"The capital of France is Paris\"  \n**Problem**: \"shakespeare\" âŒ fails to match \"William Shakespeare\" (case difference)\n\n### 2. ğŸ¤– Semantic Evaluation: `llm_as_judge`  \nThis method uses AI to understand meaning:\n- **Flexibility**: Handles paraphrasing, case differences, and context\n- **Intelligence**: Understands semantic equivalence and factual consistency\n- **Use Case**: Complex answers requiring interpretation and reasoning\n- **Cost**: Requires additional LLM inference for each evaluation\n\n**Example**: \"shakespeare\" âœ… semantically matches \"William Shakespeare wrote Romeo and Juliet\"\n\n### 3. ğŸ“Š Academic Benchmarks: Multiple Choice + Regex\nFor standardized testing:\n- **Structure**: Multiple choice questions (A, B, C, D format)\n- **Parsing**: Regex extraction of letter choices from model responses\n- **Scoring**: Binary correct/incorrect based on exact letter match\n- **Use Case**: Academic knowledge, reasoning, and comprehension testing\n\n**The Best Approach**: Use both methods together for comprehensive evaluation:\n- `subset_of` for quick sanity checks and exact factual verification\n- `llm_as_judge` for nuanced understanding and complex reasoning\n- Academic benchmarks for standardized performance comparison\n\n## ğŸ“¦ Install Required Packages\n\nInstall the evaluation dependencies:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q llama-stack-client==0.2.12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# Core imports for evaluation functionality\nfrom llama_stack_client import LlamaStackClient\nimport pprint  # Built-in Python module for pretty printing\n\n# Additional utilities for evaluation\nimport json\nimport sys"
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": "## ğŸ”— Connect to LlamaStack\n\nConnect to LlamaStack - the AI engine that orchestrates all evaluation operations. LlamaStack acts as the central hub that coordinates:\n- Model inference for generating responses to evaluation questions\n- Scoring functions to compare generated vs expected answers\n- Dataset management for organizing evaluation data\n- Benchmark execution for comprehensive testing workflows"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": "# === LlamaStack Connection Setup ===\n# The base URL points to your LlamaStack server deployment\nbase_url = \"http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321\"\n\n# Create the LlamaStack client - this is your main interface for all evaluation operations\nclient = LlamaStackClient(\n    base_url=base_url,\n    timeout=600.0  # Extended timeout for evaluation operations\n)\n\nprint(f\"ğŸ”— Connected to LlamaStack server at {base_url}\")\n\n# === Model Configuration ===\n# Get available models for evaluation\ntry:\n    available_models = [\n        model.identifier for model in client.models.list() if model.model_type == \"llm\"\n    ]\n    if available_models:\n        model_id = available_models[0]  # Use the first available model\n        print(f\"âœ… Found available model: {model_id}\")\n    else:\n        print(\"âŒ No LLM models found. Please ensure your Llama Stack has models deployed.\")\n        sys.exit(1)\nexcept Exception as e:\n    print(f\"âŒ Error connecting to LlamaStack: {e}\")\n    sys.exit(1)\n\nprint(f\"ğŸ¤– Using model: {model_id} for evaluation\")"
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": "## ğŸ¯ Step 1: Basic Evaluation with `subset_of`\n\nLet's start with the simplest evaluation method: **exact string matching**. This is the foundation of evaluation testing.\n\n**How `subset_of` works:**\n1. **Exact Search**: Looks for the expected answer as a substring within the generated response\n2. **Case Sensitive**: \"paris\" â‰  \"Paris\" - capitalization must match exactly  \n3. **Binary Scoring**: Returns 1.0 (correct) or 0.0 (incorrect)\n4. **Fast Execution**: No additional LLM calls required\n\n**Perfect for:**\n- Factual questions with specific, unambiguous answers\n- Quick sanity checks during development\n- Testing basic model functionality\n\n**Watch out for:**\n- Case sensitivity causing false negatives\n- Paraphrasing being marked as incorrect\n- Overly strict matching of essentially correct answers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# === Create Evaluation Examples ===\n# These examples will test our model's factual knowledge and demonstrate evaluation scoring\n\nhandmade_eval_rows = [\n    {\n        \"input_query\": \"What is the capital of France?\",\n        \"generated_answer\": \"The capital of France is Paris.\",\n        \"expected_answer\": \"Paris\",\n    },\n    {\n        \"input_query\": \"Who wrote Romeo and Juliet?\",\n        \"generated_answer\": \"William Shakespeare wrote Romeo and Juliet.\",\n        \"expected_answer\": \"shakespeare\",  # lowercase - this will fail due to case sensitivity!\n    },\n    {\n        \"input_query\": \"What is 2 + 2?\",\n        \"generated_answer\": \"The answer is 4.\",\n        \"expected_answer\": \"4\",\n    }\n]\n\nprint(\"ğŸ“ Testing subset_of evaluation with handmade examples:\")\nprint(\"ğŸ” Notice the case sensitivity issue in example 2...\")\npprint.pprint(handmade_eval_rows)\n\n# === Run subset_of Evaluation ===\n# This will perform exact substring matching between generated and expected answers\nprint(\"\\nğŸ§ª Running subset_of scoring...\")\n\ntry:\n    scoring_response = client.scoring.score(\n        input_rows=handmade_eval_rows,\n        scoring_functions={\"basic::subset_of\": None}  # Use default subset_of configuration\n    )\n\n    print(\"\\nğŸ“Š Raw Results:\")\n    pprint.pprint(scoring_response)\n\n    # === Analyze Results ===\n    results = scoring_response.results['basic::subset_of']\n    accuracy = results.aggregated_results['accuracy']['accuracy']\n    \n    print(f\"\\nğŸ“ˆ Overall Accuracy: {accuracy:.1%}\")\n    print(f\"âœ… Correct answers: {results.aggregated_results['accuracy']['num_correct']}\")\n    print(f\"ğŸ“Š Total questions: {results.aggregated_results['accuracy']['num_total']}\")\n    \n    # Show individual scores\n    print(\"\\nğŸ” Individual Question Analysis:\")\n    for i, (row, score_row) in enumerate(zip(handmade_eval_rows, results.score_rows)):\n        status = \"âœ… PASS\" if score_row['score'] == 1.0 else \"âŒ FAIL\"\n        print(f\"{i+1}. {status} - {row['input_query']}\")\n        print(f\"   Expected: '{row['expected_answer']}'\")\n        print(f\"   Generated: '{row['generated_answer']}'\")\n        print(f\"   Score: {score_row['score']}\")\n        \n        # Explain why Shakespeare failed\n        if score_row['score'] == 0.0 and \"shakespeare\" in row['expected_answer'].lower():\n            print(f\"   ğŸ’¡ Failed because: looking for 'shakespeare' in 'William Shakespeare...' (case mismatch)\")\n        print()\n\nexcept Exception as e:\n    print(f\"âŒ Evaluation failed: {e}\")\n    print(\"ğŸ’¡ Make sure your LlamaStack server is running and accessible\")"
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": "## ğŸ¤– Step 2: LLM-as-Judge Evaluation\n\nNow let's use a more sophisticated approach: **semantic evaluation with an LLM judge**. This addresses the case sensitivity and rigidity issues we saw with `subset_of`.\n\n**How `llm_as_judge` works:**\n1. **AI-Powered**: Uses another LLM to evaluate the semantic similarity between answers\n2. **Context-Aware**: Understands meaning, not just exact character matching\n3. **Flexible**: Handles paraphrasing, case differences, and contextual variations\n4. **Explainable**: Provides reasoning for each evaluation decision\n\n**Judge Prompt Strategy:**\nOur judge will compare factual content while ignoring style and format differences. It evaluates using these categories:\n- **(A)** Generated response is a subset of expected (correct but incomplete)\n- **(B)** Generated response is a superset of expected (correct with extra detail)  \n- **(C)** Responses contain the same factual details (perfect match)\n- **(D)** There is factual disagreement (incorrect)\n- **(E)** Answers differ but differences don't matter factually (acceptable variation)\n\n**Important Note**: In production, use a different, more capable model as the judge than the one being evaluated. For this tutorial, we're using the same model for simplicity, but this \"self-judging\" approach is not ideal for production evaluation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# === Define Judge Prompt ===\n# This prompt guides the LLM judge to make consistent, explainable evaluation decisions\nJUDGE_PROMPT = \"\"\"\nGiven a QUESTION and GENERATED_RESPONSE and EXPECTED_RESPONSE.\n\nCompare the factual content. Ignore differences in style, grammar, or punctuation.\nAnswer by selecting one option:\n(A) The GENERATED_RESPONSE is a subset of the EXPECTED_RESPONSE and is fully consistent.\n(B) The GENERATED_RESPONSE is a superset of the EXPECTED_RESPONSE and is fully consistent.\n(C) The GENERATED_RESPONSE contains all the same details as the EXPECTED_RESPONSE.\n(D) There is a disagreement between the responses.\n(E) The answers differ, but these differences don't matter factually.\n\nFormat: \"Answer: One of ABCDE, Explanation: \"\n\nQUESTION: {input_query}\nGENERATED_RESPONSE: {generated_answer}\nEXPECTED_RESPONSE: {expected_answer}\n\"\"\"\n\n# === Use Same Evaluation Examples ===\n# We'll evaluate the same examples to compare with subset_of results\nprint(f\"ğŸ¤– Testing LLM-as-judge with {model_id}:\")\nprint(\"ğŸ” Watch how the judge handles the 'shakespeare' case sensitivity issue...\")\n\n# === Run LLM-as-Judge Evaluation ===\ntry:\n    scoring_response = client.scoring.score(\n        input_rows=handmade_eval_rows,\n        scoring_functions={\n            \"llm-as-judge::base\": {\n                \"judge_model\": model_id,                              # Use available model as judge\n                \"prompt_template\": JUDGE_PROMPT,                      # Our custom evaluation prompt\n                \"type\": \"llm_as_judge\",                              # Specify the scoring type\n                \"judge_score_regexes\": [\"Answer: (A|B|C|D|E)\"],       # Extract letter grade from response\n            }\n        }\n    )\n\n    print(\"\\nğŸ“Š LLM-as-Judge Results:\")\n    results = scoring_response.results['llm-as-judge::base']\n    \n    # Display detailed results for each question\n    for i, (eval_row, score_row) in enumerate(zip(handmade_eval_rows, results.score_rows)):\n        print(f\"\\n{i+1}. Question: {eval_row['input_query']}\")\n        print(f\"   Expected: {eval_row['expected_answer']}\")\n        print(f\"   Generated: {eval_row['generated_answer']}\")\n        print(f\"   Judge Score: {score_row['score']}\")\n        print(f\"   Judge Reasoning: {score_row.get('judge_feedback', 'No feedback provided')}\")\n        \n        # Interpret the judge's decision\n        if score_row['score'] in ['A', 'B', 'C', 'E']:\n            print(\"   ğŸ¯ Judge Decision: âœ… SEMANTICALLY CORRECT\")\n        else:\n            print(\"   ğŸ¯ Judge Decision: âŒ FACTUALLY INCORRECT\")\n\n    print(f\"\\nğŸ§  Key Insight: Notice how the LLM judge handles 'shakespeare' vs 'William Shakespeare'\")\n    print(f\"   The judge understands these refer to the same person, unlike exact string matching!\")\n\nexcept Exception as e:\n    print(f\"âŒ LLM-as-judge evaluation failed: {e}\")\n    print(\"ğŸ’¡ This might be due to model availability or prompt formatting issues\")"
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": "## ğŸ“š Step 3: Dataset-based Evaluation with SimpleQA\n\nNow let's move beyond handcrafted examples to **real-world datasets**. This tests how models perform on questions they haven't seen before.\n\n**SimpleQA Dataset:**\n- **Source**: Research-grade dataset from HuggingFace/Meta\n- **Content**: Factual knowledge questions across diverse topics\n- **Difficulty**: Tests specific, verifiable facts that require precise knowledge\n- **Examples**: \"Who received the IEEE Frank Rosenblatt Award in 2010?\"\n\n**Why Dataset Evaluation Matters:**\n1. **Unbiased Testing**: Questions you didn't create, reducing selection bias\n2. **Scale**: Test across hundreds or thousands of examples automatically\n3. **Benchmarking**: Compare your model's performance to published baselines\n4. **Real-world Readiness**: Evaluate on the types of questions users actually ask\n\n**Evaluation Workflow:**\n1. **Register Dataset**: Connect to external data source (HuggingFace)\n2. **Sample Questions**: Get a subset for quick testing\n3. **Model Inference**: Generate answers to dataset questions\n4. **LLM-as-Judge**: Use semantic evaluation for nuanced comparison\n5. **Analyze Results**: Identify patterns in successes and failures"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# === Step 3: Dataset-based Evaluation ===\n\nprint(\"ğŸ“š Setting up SimpleQA dataset evaluation...\")\n\ntry:\n    # === Register SimpleQA Dataset ===\n    # This connects to the HuggingFace dataset of factual knowledge questions\n    print(\"ğŸ”— Registering SimpleQA dataset...\")\n    client.datasets.register(\n        purpose=\"eval/messages-answer\",                                      # Dataset format type\n        source={\n            \"type\": \"uri\",\n            \"uri\": \"huggingface://datasets/llamastack/simpleqa?split=train\", # HuggingFace dataset URI\n        },\n        dataset_id=\"huggingface::simpleqa\",                                 # Local identifier\n    )\n    print(\"âœ… Dataset registered successfully!\")\n\n    # === Sample Questions from Dataset ===\n    # Get a small sample for quick testing (you can increase limit for more comprehensive testing)\n    print(\"\\nğŸ“‹ Sampling questions from SimpleQA dataset...\")\n    eval_rows = client.datasets.iterrows(\n        dataset_id=\"huggingface::simpleqa\",\n        limit=3,  # Start small for demo - increase to 10-50 for real evaluation\n    )\n\n    print(\"\\nğŸ” Sample questions from SimpleQA:\")\n    for i, row in enumerate(eval_rows.data):\n        print(f\"{i+1}. {row['input_query']}\")\n        print(f\"   Expected: {row['expected_answer']}\")\n        print()\n\n    # === Create Benchmark ===\n    # This defines how we'll evaluate the dataset\n    print(\"ğŸ¯ Creating benchmark with LLM-as-judge scoring...\")\n    client.benchmarks.register(\n        benchmark_id=\"meta-reference::simpleqa\",\n        dataset_id=\"huggingface::simpleqa\",\n        scoring_functions=[\"llm-as-judge::base\"],  # Use our semantic evaluation\n    )\n\n    # === Evaluate Model on Dataset ===\n    print(f\"ğŸ¤– Evaluating {model_id} on SimpleQA knowledge questions...\")\n    print(\"â³ This may take a moment as we generate answers and judge them...\")\n    \n    response = client.eval.evaluate_rows(\n        benchmark_id=\"meta-reference::simpleqa\",\n        input_rows=eval_rows.data,\n        scoring_functions=[\"llm-as-judge::base\"],\n        benchmark_config={\n            \"eval_candidate\": {\n                \"type\": \"model\",\n                \"model\": model_id,\n                \"sampling_params\": {\n                    \"strategy\": {\"type\": \"greedy\"},  # Deterministic generation\n                    \"max_tokens\": 512,               # Allow detailed answers\n                },\n            },\n        },\n    )\n\n    # === Analyze Dataset Results ===\n    print(\"\\nğŸ“Š SimpleQA Evaluation Results:\")\n    print(\"=\" * 60)\n    \n    for i, gen in enumerate(response.generations):\n        score = response.scores['llm-as-judge::base'].score_rows[i]\n        \n        print(f\"\\n{i+1}. Question: {eval_rows.data[i]['input_query']}\")\n        print(f\"   Expected: {eval_rows.data[i]['expected_answer']}\")\n        print(f\"   Generated: {gen['generated_answer']}\")\n        print(f\"   Judge Score: {score['score']}\")\n        print(f\"   Judge Feedback: {score.get('judge_feedback', 'No feedback')}\")\n        \n        # Categorize performance\n        if score['score'] in ['A', 'B', 'C', 'E']:\n            print(\"   ğŸ“ˆ Result: âœ… CORRECT/ACCEPTABLE\")\n        else:\n            print(\"   ğŸ“‰ Result: âŒ INCORRECT/INADEQUATE\")\n\n    # === Performance Summary ===\n    correct_count = sum(1 for score in response.scores['llm-as-judge::base'].score_rows \n                       if score['score'] in ['A', 'B', 'C', 'E'])\n    total_count = len(response.scores['llm-as-judge::base'].score_rows)\n    accuracy = correct_count / total_count if total_count > 0 else 0\n\n    print(f\"\\nğŸ¯ Dataset Performance Summary:\")\n    print(f\"   Correct: {correct_count}/{total_count} ({accuracy:.1%})\")\n    print(f\"   Model: {model_id}\")\n    print(f\"   Dataset: SimpleQA (factual knowledge)\")\n\nexcept Exception as e:\n    print(f\"âŒ Dataset evaluation failed: {e}\")\n    print(\"ğŸ’¡ This might be due to dataset availability or network connectivity issues\")"
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": "## ğŸ“ Step 4: Academic Benchmark (MMLU-style) Evaluation\n\nLet's test on **standardized academic benchmarks** - the gold standard for comparing model capabilities across institutions and research papers.\n\n**MMLU (Massive Multitask Language Understanding):**\n- **Format**: Multiple choice questions (A, B, C, D)\n- **Subjects**: 57 academic domains (math, history, science, law, etc.)\n- **Difficulty**: High school to graduate level knowledge\n- **Scoring**: Binary correct/incorrect based on letter choice\n\n**Key Differences from Previous Methods:**\n- **Structured Output**: Must extract specific letter choice (A/B/C/D)\n- **System Prompts**: Use academic expert personas for better performance\n- **Regex Parsing**: Extract final answer from potentially verbose explanations\n- **Standardized**: Results comparable to published research\n\n**Why Academic Benchmarks Matter:**\n1. **Research Comparability**: Your results can be compared to published papers\n2. **Objective Metrics**: Clear pass/fail criteria with no ambiguity\n3. **Comprehensive Coverage**: Tests reasoning across multiple domains\n4. **Industry Standard**: Used by major AI companies for model evaluation"
  },
  {
   "cell_type": "code",
   "id": "u5t8jxp4dva",
   "source": "# === Step 4: Academic Benchmark (MMLU-style) Evaluation ===\n\n# === Define System Prompt for Academic Excellence ===\n# This prompt helps the model perform better on academic questions by setting the right context\nSYSTEM_PROMPT_TEMPLATE = \"\"\"\nYou are an expert in {subject} whose job is to answer multiple choice questions.\n\nFirst, reason about the correct answer.\n\nThen write the answer in the following format where X is exactly one of A,B,C,D:\n\nAnswer: X\n\nMake sure X is one of A,B,C,D.\n\nIf you are uncertain of the correct answer, guess the most likely one.\n\"\"\"\n\n# === Sample MMLU-style Questions ===\n# In a real evaluation, you'd load thousands of questions from the actual MMLU dataset\nmmlu_sample_rows = [\n    {\n        \"input_query\": \"What is the capital of France?\\nA) London\\nB) Berlin\\nC) Paris\\nD) Madrid\",\n        \"expected_answer\": \"C\",\n        \"chat_completion_input\": '[{\"role\": \"user\", \"content\": \"What is the capital of France?\\\\nA) London\\\\nB) Berlin\\\\nC) Paris\\\\nD) Madrid\"}]'\n    },\n    {\n        \"input_query\": \"Which of the following is a prime number?\\nA) 4\\nB) 6\\nC) 8\\nD) 7\",\n        \"expected_answer\": \"D\",\n        \"chat_completion_input\": '[{\"role\": \"user\", \"content\": \"Which of the following is a prime number?\\\\nA) 4\\\\nB) 6\\\\nC) 8\\\\nD) 7\"}]'\n    },\n    {\n        \"input_query\": \"Who wrote 'Romeo and Juliet'?\\nA) Charles Dickens\\nB) William Shakespeare\\nC) Mark Twain\\nD) Jane Austen\",\n        \"expected_answer\": \"B\",\n        \"chat_completion_input\": '[{\"role\": \"user\", \"content\": \"Who wrote \\'Romeo and Juliet\\'?\\\\nA) Charles Dickens\\\\nB) William Shakespeare\\\\nC) Mark Twain\\\\nD) Jane Austen\"}]'\n    }\n]\n\nprint(\"ğŸ“ MMLU-style Multiple Choice Evaluation\")\nprint(\"ğŸ“š Testing academic knowledge with structured multiple choice format...\")\n\nfor i, row in enumerate(mmlu_sample_rows):\n    print(f\"\\n{i+1}. {row['input_query']}\")\n    print(f\"   Correct Answer: {row['expected_answer']}\")\n\n# === Create Academic Expert System Message ===\nsystem_message = {\n    \"role\": \"system\", \n    \"content\": SYSTEM_PROMPT_TEMPLATE.format(subject=\"academic subjects\"),\n}\n\nprint(f\"\\nğŸ§ª Running MMLU-style evaluation with {model_id}...\")\n\ntry:\n    # === Register MMLU-style Benchmark ===\n    client.benchmarks.register(\n        benchmark_id=\"meta-reference::mmlu-sample\",\n        dataset_id=\"mmlu-sample\", \n        scoring_functions=[],  # We'll use regex parser for multiple choice\n    )\n\n    # === Evaluate with Regex Parser for Multiple Choice ===\n    # This extracts the letter choice (A/B/C/D) from the model's response\n    response = client.eval.evaluate_rows(\n        benchmark_id=\"meta-reference::mmlu-sample\",\n        input_rows=mmlu_sample_rows,\n        scoring_functions=[\"basic::regex_parser_multiple_choice_answer\"],\n        benchmark_config={\n            \"eval_candidate\": {\n                \"type\": \"model\",\n                \"model\": model_id,\n                \"sampling_params\": {\n                    \"strategy\": {\n                        \"type\": \"top_p\",\n                        \"temperature\": 0.1,  # Slightly creative but mostly deterministic\n                        \"top_p\": 0.95,\n                    },\n                    \"max_tokens\": 512,\n                },\n                \"system_message\": system_message,  # Use our academic expert prompt\n            },\n        },\n    )\n\n    # === Analyze MMLU Results ===\n    print(\"\\nğŸ“Š MMLU-style Evaluation Results:\")\n    print(\"=\" * 70)\n    \n    for i, gen in enumerate(response.generations):\n        score = response.scores['basic::regex_parser_multiple_choice_answer'].score_rows[i]\n        \n        question_short = mmlu_sample_rows[i]['input_query'].split('?')[0] + \"?\"\n        print(f\"\\n{i+1}. Question: {question_short}\")\n        print(f\"   Expected: {mmlu_sample_rows[i]['expected_answer']}\")\n        print(f\"   Generated Response: {gen['generated_answer']}\")\n        print(f\"   Extracted Answer: {score.get('parsed_answer', 'Failed to parse')}\")\n        print(f\"   Score: {'âœ… CORRECT' if score['score'] == 1.0 else 'âŒ INCORRECT'} ({score['score']})\")\n\n    # === Calculate MMLU Accuracy ===\n    results = response.scores['basic::regex_parser_multiple_choice_answer']\n    if 'accuracy' in results.aggregated_results:\n        accuracy = results.aggregated_results['accuracy']['accuracy']\n        correct = results.aggregated_results['accuracy']['num_correct']\n        total = results.aggregated_results['accuracy']['num_total']\n        \n        print(f\"\\nğŸ¯ MMLU-style Performance Summary:\")\n        print(f\"   Accuracy: {accuracy:.1%} ({correct}/{total})\")\n        print(f\"   Model: {model_id}\")\n        print(f\"   Format: Multiple Choice (A/B/C/D)\")\n        print(f\"   Parsing: Regex extraction of answer letters\")\n        \n        # Provide context for the results\n        if accuracy >= 0.8:\n            print(f\"   ğŸŒŸ Excellent performance! This model handles structured academic questions well.\")\n        elif accuracy >= 0.6:\n            print(f\"   ğŸ“ˆ Good performance with room for improvement on academic reasoning.\")\n        else:\n            print(f\"   ğŸ“‰ Consider fine-tuning or using more capable models for academic benchmarks.\")\n    else:\n        print(\"   âš ï¸  Could not calculate aggregated accuracy metrics\")\n\nexcept Exception as e:\n    print(f\"âŒ MMLU evaluation failed: {e}\")\n    print(\"ğŸ’¡ This might be due to parsing issues or model response formatting problems\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ujaam5dzna",
   "source": "## ğŸ‰ Comprehensive Evaluation Complete!\n\n**What you accomplished:**\n- **ğŸ¯ Basic Evaluation**: Implemented exact string matching with `subset_of` for fast, precise testing\n- **ğŸ¤– Semantic Evaluation**: Used `llm_as_judge` for flexible, context-aware evaluation with explanations\n- **ğŸ“š Dataset Testing**: Evaluated on real-world SimpleQA dataset for unbiased performance measurement\n- **ğŸ“ Academic Benchmarks**: Tested structured multiple-choice questions with regex parsing for standardized comparison\n- **ğŸ“Š End-to-End Pipeline**: Built complete evaluation workflows from question to performance metrics\n\n**Key Technical Insights:**\n- **Exact vs Semantic**: `subset_of` is fast but rigid; `llm_as_judge` is flexible but requires more resources\n- **Dataset Diversity**: Real datasets reveal different failure modes than handcrafted examples  \n- **Structured Evaluation**: Academic benchmarks provide standardized, comparable performance metrics\n- **Error Analysis**: Each method reveals different aspects of model capabilities and limitations\n\n**Evaluation Method Comparison:**\n| Method | Speed | Flexibility | Use Case | Cost |\n|--------|-------|-------------|----------|------|\n| `subset_of` | âš¡ Fast | ğŸ”’ Rigid | Quick sanity checks | ğŸ’° Free |\n| `llm_as_judge` | ğŸŒ Slower | ğŸ¤¸ Flexible | Nuanced evaluation | ğŸ’°ğŸ’° LLM calls |\n| Multiple Choice + Regex | âš¡ Fast | ğŸ“ Structured | Academic benchmarks | ğŸ’° Model calls only |\n\n**Production Best Practices:**\n1. **Multi-Method Approach**: Use all three methods together for comprehensive coverage\n2. **Judge Model Selection**: Use stronger models (Llama 3.3 70B, 405B) as judges, not the model being evaluated\n3. **Dataset Rotation**: Regularly test on new, unseen datasets to prevent overfitting\n4. **Error Analysis**: Investigate failure patterns to guide model improvements\n5. **Benchmark Tracking**: Monitor performance trends over time and model versions\n\n**Advanced Evaluation Patterns to Explore:**\n- **Multi-turn Conversations**: Evaluate dialogue consistency and context retention\n- **Tool Use Evaluation**: Test agent capabilities with external tools and APIs\n- **Safety & Alignment**: Evaluate harmful content detection and refusal behaviors\n- **Domain-specific Benchmarks**: Create custom evaluation datasets for your specific use case\n- **Human Evaluation**: Combine automated metrics with human judgment for complex tasks\n\n**Next Steps:**\n- Scale up evaluation with larger datasets (100s-1000s of examples)\n- Implement automated evaluation pipelines for continuous testing\n- Create custom evaluation datasets for your specific domain or use case\n- Set up performance monitoring and alerting for production model deployment\n\nYour evaluation framework is now production-ready - the foundation for deploying AI systems with confidence! ğŸš€",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}