{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": "# ğŸ§ª Evaluating Llama Stack Models with Financial Examples\n\nThis notebook demonstrates **Model Evaluation** using simple Financial Services examples - testing AI models on basic financial knowledge and concepts.\n\n**What is Model Evaluation?**\nModel evaluation helps ensure AI models work correctly by:\n- **Testing** models against known financial facts and definitions\n- **Measuring** performance on basic financial concepts\n- **Identifying** issues before deployment\n- **Comparing** different models\n\n**Why Evaluation Matters for Finance:**\n- **Accuracy**: Verify financial knowledge is correct\n- **Trust**: Ensure reliable answers about financial concepts\n- **Quality**: Test on fundamental financial knowledge\n\n**Note**: We use historical/general financial knowledge, not current market data that requires real-time tools.\n\n**Llama Stack's Evaluation Framework:**\nLlama Stack provides simple APIs for evaluation:\n- ğŸ“Š **`/datasetio` + `/datasets`** - Managing datasets\n- ğŸ¯ **`/scoring` + `/scoring_functions`** - Running scoring  \n- ğŸ“ˆ **`/eval` + `/benchmarks`** - Evaluation workflows\n\nLet's test with simple financial knowledge! ğŸ’°ğŸš€"
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": "## ğŸ—ï¸ Understanding Evaluation Methods\n\nLlamaStack provides **simple evaluation approaches** for testing financial knowledge:\n\n### 1. ğŸ¯ Exact Matching: `subset_of`\nThis method checks for exact matches:\n- **Speed**: Very fast\n- **Use Case**: Perfect for checking specific financial definitions\n- **Example**: \"Initial Public Offering\" âœ… matches \"IPO stands for Initial Public Offering\"\n\n### 2. ğŸ¤– Semantic Evaluation: `llm_as_judge`  \nThis method uses AI to understand meaning:\n- **Flexibility**: Handles different ways of saying the same thing\n- **Use Case**: More complex financial explanations\n- **Example**: \"Warren Buffett\" âœ… matches \"the Oracle of Omaha\"\n\n### 3. ğŸ“Š Multiple Choice Questions\nFor standardized testing:\n- **Format**: A, B, C, D questions\n- **Use Case**: Testing financial terminology and concepts\n\n**The Best Approach**: Use all methods together for complete testing.\n\n## ğŸ“¦ Install Required Packages\n\nInstall the evaluation dependencies:"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-stack-client==0.2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core imports for evaluation functionality\n",
    "from llama_stack_client import LlamaStackClient\n",
    "import pprint  # Built-in Python module for pretty printing\n",
    "\n",
    "# Additional utilities for evaluation\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "## ğŸ”— Connect to LlamaStack\n",
    "\n",
    "Connect to LlamaStack - the AI engine that orchestrates all evaluation operations. LlamaStack acts as the central hub that coordinates:\n",
    "- Model inference for generating responses to evaluation questions\n",
    "- Scoring functions to compare generated vs expected answers\n",
    "- Dataset management for organizing evaluation data\n",
    "- Benchmark execution for comprehensive testing workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Connected to LlamaStack server at http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321\n",
      "âœ… Found available model: llama3-2-3b\n",
      "ğŸ¤– Using model: llama3-2-3b for evaluation\n"
     ]
    }
   ],
   "source": [
    "# === LlamaStack Connection Setup ===\n",
    "# The base URL points to your LlamaStack server deployment\n",
    "base_url = \"http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321\"\n",
    "\n",
    "# Create the LlamaStack client - this is your main interface for all evaluation operations\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    timeout=600.0  # Extended timeout for evaluation operations\n",
    ")\n",
    "\n",
    "print(f\"ğŸ”— Connected to LlamaStack server at {base_url}\")\n",
    "\n",
    "# === Model Configuration ===\n",
    "# Get available models for evaluation\n",
    "try:\n",
    "    available_models = [\n",
    "        model.identifier for model in client.models.list() if model.model_type == \"llm\"\n",
    "    ]\n",
    "    if available_models:\n",
    "        model_id = available_models[0]  # Use the first available model\n",
    "        print(f\"âœ… Found available model: {model_id}\")\n",
    "    else:\n",
    "        print(\"âŒ No LLM models found. Please ensure your Llama Stack has models deployed.\")\n",
    "        sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to LlamaStack: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"ğŸ¤– Using model: {model_id} for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 1: Basic Evaluation with `subset_of`\n",
    "\n",
    "Let's start with the simplest evaluation method: **exact string matching**. This is the foundation of evaluation testing.\n",
    "\n",
    "**How `subset_of` works:**\n",
    "1. **Exact Search**: Looks for the expected answer as a substring within the generated response\n",
    "2. **Case Sensitive**: \"paris\" â‰  \"Paris\" - capitalization must match exactly  \n",
    "3. **Binary Scoring**: Returns 1.0 (correct) or 0.0 (incorrect)\n",
    "4. **Fast Execution**: No additional LLM calls required\n",
    "\n",
    "**Perfect for:**\n",
    "- Factual questions with specific, unambiguous answers\n",
    "- Quick sanity checks during development\n",
    "- Testing basic model functionality\n",
    "\n",
    "**Watch out for:**\n",
    "- Case sensitivity causing false negatives\n",
    "- Paraphrasing being marked as incorrect\n",
    "- Overly strict matching of essentially correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# === Create Simple FSI Evaluation Examples ===\n# These examples test basic historical financial knowledge\n\nhandmade_eval_rows = [\n    {\n        \"input_query\": \"What does IPO stand for?\",\n        \"generated_answer\": \"IPO stands for Initial Public Offering.\",\n        \"expected_answer\": \"Initial Public Offering\",\n    },\n    {\n        \"input_query\": \"What company was founded by Bill Gates?\",\n        \"generated_answer\": \"Microsoft was founded by Bill Gates.\",\n        \"expected_answer\": \"Microsoft\",\n    },\n    {\n        \"input_query\": \"What does ROI stand for in finance?\",\n        \"generated_answer\": \"ROI stands for Return on Investment.\",\n        \"expected_answer\": \"Return on Investment\",\n    }\n]\n\nprint(\"ğŸ’° Testing subset_of evaluation with simple FSI examples:\")\nprint(\"ğŸ” Notice how we test basic financial knowledge...\")\npprint.pprint(handmade_eval_rows)\n\n# === Run subset_of Evaluation ===\n# This will perform exact substring matching between generated and expected answers\nprint(\"\\nğŸ§ª Running subset_of scoring...\")\n\ntry:\n    scoring_response = client.scoring.score(\n        input_rows=handmade_eval_rows,\n        scoring_functions={\"basic::subset_of\": None}  # Use default subset_of configuration\n    )\n\n    print(\"\\nğŸ“Š Raw Results:\")\n    pprint.pprint(scoring_response)\n\n    # === Analyze Results ===\n    results = scoring_response.results['basic::subset_of']\n    accuracy = results.aggregated_results['accuracy']['accuracy']\n    \n    print(f\"\\nğŸ“ˆ Overall Accuracy: {accuracy:.1%}\")\n    print(f\"âœ… Correct answers: {results.aggregated_results['accuracy']['num_correct']}\")\n    print(f\"ğŸ“Š Total questions: {results.aggregated_results['accuracy']['num_total']}\")\n    \n    # Show individual scores\n    print(\"\\nğŸ” Individual FSI Question Analysis:\")\n    for i, (row, score_row) in enumerate(zip(handmade_eval_rows, results.score_rows)):\n        status = \"âœ… PASS\" if score_row['score'] == 1.0 else \"âŒ FAIL\"\n        print(f\"{i+1}. {status} - {row['input_query']}\")\n        print(f\"   Expected: '{row['expected_answer']}'\")\n        print(f\"   Generated: '{row['generated_answer']}'\")\n        print(f\"   Score: {score_row['score']}\")\n        print()\n\nexcept Exception as e:\n    print(f\"âŒ Evaluation failed: {e}\")\n    print(\"ğŸ’¡ Make sure your LlamaStack server is running and accessible\")"
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 2: LLM-as-Judge Evaluation\n",
    "\n",
    "Now let's use a more sophisticated approach: **semantic evaluation with an LLM judge**. This addresses the case sensitivity and rigidity issues we saw with `subset_of`.\n",
    "\n",
    "**How `llm_as_judge` works:**\n",
    "1. **AI-Powered**: Uses another LLM to evaluate the semantic similarity between answers\n",
    "2. **Context-Aware**: Understands meaning, not just exact character matching\n",
    "3. **Flexible**: Handles paraphrasing, case differences, and contextual variations\n",
    "4. **Explainable**: Provides reasoning for each evaluation decision\n",
    "\n",
    "**Judge Prompt Strategy:**\n",
    "Our judge will compare factual content while ignoring style and format differences. It evaluates using these categories:\n",
    "- **(A)** Generated response is a subset of expected (correct but incomplete)\n",
    "- **(B)** Generated response is a superset of expected (correct with extra detail)  \n",
    "- **(C)** Responses contain the same factual details (perfect match)\n",
    "- **(D)** There is factual disagreement (incorrect)\n",
    "- **(E)** Answers differ but differences don't matter factually (acceptable variation)\n",
    "\n",
    "**Important Note**: In production, use a different, more capable model as the judge than the one being evaluated. For this tutorial, we're using the same model for simplicity, but this \"self-judging\" approach is not ideal for production evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Testing LLM-as-judge with llama3-2-3b:\n",
      "ğŸ” Watch how the judge handles the 'shakespeare' case sensitivity issue...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/scoring/score \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LLM-as-Judge Results:\n",
      "\n",
      "1. Question: What is the capital of France?\n",
      "   Expected: Paris\n",
      "   Generated: The capital of France is Paris.\n",
      "   Judge Score: C\n",
      "   Judge Reasoning: Answer: C, Explanation: The GENERATED_RESPONSE and EXPECTED_RESPONSE contain the same factual information, specifically that the capital of France is Paris.\n",
      "   ğŸ¯ Judge Decision: âœ… SEMANTICALLY CORRECT\n",
      "\n",
      "2. Question: Who wrote Romeo and Juliet?\n",
      "   Expected: shakespeare\n",
      "   Generated: William Shakespeare wrote Romeo and Juliet.\n",
      "   Judge Score: B\n",
      "   Judge Reasoning: Answer: B, Explanation: The GENERATED_RESPONSE is a superset of the EXPECTED_RESPONSE and is fully consistent.\n",
      "   ğŸ¯ Judge Decision: âœ… SEMANTICALLY CORRECT\n",
      "\n",
      "3. Question: What is 2 + 2?\n",
      "   Expected: 4\n",
      "   Generated: The answer is 4.\n",
      "   Judge Score: C\n",
      "   Judge Reasoning: Answer: C, Explanation: The GENERATED_RESPONSE and EXPECTED_RESPONSE contain the same factual content, which is the answer to the question \"What is 2 + 2?\", and are fully consistent.\n",
      "   ğŸ¯ Judge Decision: âœ… SEMANTICALLY CORRECT\n",
      "\n",
      "ğŸ§  Key Insight: Notice how the LLM judge handles 'shakespeare' vs 'William Shakespeare'\n",
      "   The judge understands these refer to the same person, unlike exact string matching!\n"
     ]
    }
   ],
   "source": [
    "# === Define Judge Prompt ===\n",
    "# This prompt guides the LLM judge to make consistent, explainable evaluation decisions\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "Given a QUESTION and GENERATED_RESPONSE and EXPECTED_RESPONSE.\n",
    "\n",
    "Compare the factual content. Ignore differences in style, grammar, or punctuation.\n",
    "Answer by selecting one option:\n",
    "(A) The GENERATED_RESPONSE is a subset of the EXPECTED_RESPONSE and is fully consistent.\n",
    "(B) The GENERATED_RESPONSE is a superset of the EXPECTED_RESPONSE and is fully consistent.\n",
    "(C) The GENERATED_RESPONSE contains all the same details as the EXPECTED_RESPONSE.\n",
    "(D) There is a disagreement between the responses.\n",
    "(E) The answers differ, but these differences don't matter factually.\n",
    "\n",
    "Format: \"Answer: One of ABCDE, Explanation: \"\n",
    "\n",
    "QUESTION: {input_query}\n",
    "GENERATED_RESPONSE: {generated_answer}\n",
    "EXPECTED_RESPONSE: {expected_answer}\n",
    "\"\"\"\n",
    "\n",
    "# === Use Same Evaluation Examples ===\n",
    "# We'll evaluate the same examples to compare with subset_of results\n",
    "print(f\"ğŸ¤– Testing LLM-as-judge with {model_id}:\")\n",
    "print(\"ğŸ” Watch how the judge handles the 'shakespeare' case sensitivity issue...\")\n",
    "\n",
    "# === Run LLM-as-Judge Evaluation ===\n",
    "try:\n",
    "    scoring_response = client.scoring.score(\n",
    "        input_rows=handmade_eval_rows,\n",
    "        scoring_functions={\n",
    "            \"llm-as-judge::base\": {\n",
    "                \"judge_model\": model_id,                              # Use available model as judge\n",
    "                \"prompt_template\": JUDGE_PROMPT,                      # Our custom evaluation prompt\n",
    "                \"type\": \"llm_as_judge\",                              # Specify the scoring type\n",
    "                \"judge_score_regexes\": [\"Answer: (A|B|C|D|E)\"],       # Extract letter grade from response\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ“Š LLM-as-Judge Results:\")\n",
    "    results = scoring_response.results['llm-as-judge::base']\n",
    "    \n",
    "    # Display detailed results for each question\n",
    "    for i, (eval_row, score_row) in enumerate(zip(handmade_eval_rows, results.score_rows)):\n",
    "        print(f\"\\n{i+1}. Question: {eval_row['input_query']}\")\n",
    "        print(f\"   Expected: {eval_row['expected_answer']}\")\n",
    "        print(f\"   Generated: {eval_row['generated_answer']}\")\n",
    "        print(f\"   Judge Score: {score_row['score']}\")\n",
    "        print(f\"   Judge Reasoning: {score_row.get('judge_feedback', 'No feedback provided')}\")\n",
    "        \n",
    "        # Interpret the judge's decision\n",
    "        if score_row['score'] in ['A', 'B', 'C', 'E']:\n",
    "            print(\"   ğŸ¯ Judge Decision: âœ… SEMANTICALLY CORRECT\")\n",
    "        else:\n",
    "            print(\"   ğŸ¯ Judge Decision: âŒ FACTUALLY INCORRECT\")\n",
    "\n",
    "    print(f\"\\nğŸ§  Key Insight: Notice how the LLM judge handles 'shakespeare' vs 'William Shakespeare'\")\n",
    "    print(f\"   The judge understands these refer to the same person, unlike exact string matching!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ LLM-as-judge evaluation failed: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to model availability or prompt formatting issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 3: Dataset-based Evaluation with SimpleQA\n",
    "\n",
    "Now let's move beyond handcrafted examples to **real-world datasets**. This tests how models perform on questions they haven't seen before.\n",
    "\n",
    "**SimpleQA Dataset:**\n",
    "- **Source**: Research-grade dataset from HuggingFace/Meta\n",
    "- **Content**: Factual knowledge questions across diverse topics\n",
    "- **Difficulty**: Tests specific, verifiable facts that require precise knowledge\n",
    "- **Examples**: \"Who received the IEEE Frank Rosenblatt Award in 2010?\"\n",
    "\n",
    "**Why Dataset Evaluation Matters:**\n",
    "1. **Unbiased Testing**: Questions you didn't create, reducing selection bias\n",
    "2. **Scale**: Test across hundreds or thousands of examples automatically\n",
    "3. **Benchmarking**: Compare your model's performance to published baselines\n",
    "4. **Real-world Readiness**: Evaluate on the types of questions users actually ask\n",
    "\n",
    "**Evaluation Workflow:**\n",
    "1. **Register Dataset**: Connect to external data source (HuggingFace)\n",
    "2. **Sample Questions**: Get a subset for quick testing\n",
    "3. **Model Inference**: Generate answers to dataset questions\n",
    "4. **LLM-as-Judge**: Use semantic evaluation for nuanced comparison\n",
    "5. **Analyze Results**: Identify patterns in successes and failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/datasets \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Setting up SimpleQA dataset evaluation...\n",
      "ğŸ”— Registering SimpleQA dataset...\n",
      "âœ… Dataset registered successfully!\n",
      "\n",
      "ğŸ“‹ Sampling questions from SimpleQA dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/datasetio/iterrows/huggingface::simpleqa?limit=3 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/eval/benchmarks \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Sample questions from SimpleQA:\n",
      "1. Who received the IEEE Frank Rosenblatt Award in 2010?\n",
      "   Expected: Michio Sugeno\n",
      "\n",
      "2. Who was awarded the Oceanography Society's Jerlov Award in 2018?\n",
      "   Expected: Annick Bricaud\n",
      "\n",
      "3. What's the name of the women's liberal arts college in Cambridge, Massachusetts?\n",
      "   Expected: Radcliffe College\n",
      "\n",
      "ğŸ¯ Creating benchmark with LLM-as-judge scoring...\n",
      "ğŸ¤– Evaluating llama3-2-3b on SimpleQA knowledge questions...\n",
      "â³ This may take a moment as we generate answers and judge them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/eval/benchmarks/meta-reference::simpleqa/evaluations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š SimpleQA Evaluation Results:\n",
      "============================================================\n",
      "\n",
      "1. Question: Who received the IEEE Frank Rosenblatt Award in 2010?\n",
      "   Expected: Michio Sugeno\n",
      "   Generated: The IEEE Frank Rosenblatt Award in 2010 was received by Yann LeCun.\n",
      "   Judge Score: D\n",
      "   Judge Feedback: Answer: D, Explanation: There is a disagreement between the responses.\n",
      "   ğŸ“‰ Result: âŒ INCORRECT/INADEQUATE\n",
      "\n",
      "2. Question: Who was awarded the Oceanography Society's Jerlov Award in 2018?\n",
      "   Expected: Annick Bricaud\n",
      "   Generated: I couldn't find any information on the Oceanography Society's Jerlov Award.\n",
      "   Judge Score: D\n",
      "   Judge Feedback: Answer: D, Explanation: There is a disagreement between the responses.\n",
      "   ğŸ“‰ Result: âŒ INCORRECT/INADEQUATE\n",
      "\n",
      "3. Question: What's the name of the women's liberal arts college in Cambridge, Massachusetts?\n",
      "   Expected: Radcliffe College\n",
      "   Generated: The women's liberal arts college in Cambridge, Massachusetts is Wellesley College.\n",
      "   Judge Score: C\n",
      "   Judge Feedback: Answer: C, Explanation: The GENERATED_RESPONSE contains all the same details as the EXPECTED_RESPONSE, specifically the name of the women's liberal arts college in Cambridge, Massachusetts.\n",
      "   ğŸ“ˆ Result: âœ… CORRECT/ACCEPTABLE\n",
      "\n",
      "ğŸ¯ Dataset Performance Summary:\n",
      "   Correct: 1/3 (33.3%)\n",
      "   Model: llama3-2-3b\n",
      "   Dataset: SimpleQA (factual knowledge)\n"
     ]
    }
   ],
   "source": [
    "# === Step 3: Dataset-based Evaluation ===\n",
    "\n",
    "print(\"ğŸ“š Setting up SimpleQA dataset evaluation...\")\n",
    "\n",
    "try:\n",
    "    # === Register SimpleQA Dataset ===\n",
    "    # This connects to the HuggingFace dataset of factual knowledge questions\n",
    "    print(\"ğŸ”— Registering SimpleQA dataset...\")\n",
    "    client.datasets.register(\n",
    "        purpose=\"eval/messages-answer\",                                      # Dataset format type\n",
    "        source={\n",
    "            \"type\": \"uri\",\n",
    "            \"uri\": \"huggingface://datasets/llamastack/simpleqa?split=train\", # HuggingFace dataset URI\n",
    "        },\n",
    "        dataset_id=\"huggingface::simpleqa\",                                 # Local identifier\n",
    "    )\n",
    "    print(\"âœ… Dataset registered successfully!\")\n",
    "\n",
    "    # === Sample Questions from Dataset ===\n",
    "    # Get a small sample for quick testing (you can increase limit for more comprehensive testing)\n",
    "    print(\"\\nğŸ“‹ Sampling questions from SimpleQA dataset...\")\n",
    "    eval_rows = client.datasets.iterrows(\n",
    "        dataset_id=\"huggingface::simpleqa\",\n",
    "        limit=3,  # Start small for demo - increase to 10-50 for real evaluation\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ” Sample questions from SimpleQA:\")\n",
    "    for i, row in enumerate(eval_rows.data):\n",
    "        print(f\"{i+1}. {row['input_query']}\")\n",
    "        print(f\"   Expected: {row['expected_answer']}\")\n",
    "        print()\n",
    "\n",
    "    # === Create Benchmark ===\n",
    "    # This defines how we'll evaluate the dataset\n",
    "    print(\"ğŸ¯ Creating benchmark with LLM-as-judge scoring...\")\n",
    "    client.benchmarks.register(\n",
    "        benchmark_id=\"meta-reference::simpleqa\",\n",
    "        dataset_id=\"huggingface::simpleqa\",\n",
    "        scoring_functions=[\"llm-as-judge::base\"],  # Use our semantic evaluation\n",
    "    )\n",
    "\n",
    "    # === Evaluate Model on Dataset ===\n",
    "    print(f\"ğŸ¤– Evaluating {model_id} on SimpleQA knowledge questions...\")\n",
    "    print(\"â³ This may take a moment as we generate answers and judge them...\")\n",
    "    \n",
    "    response = client.eval.evaluate_rows(\n",
    "        benchmark_id=\"meta-reference::simpleqa\",\n",
    "        input_rows=eval_rows.data,\n",
    "        scoring_functions=[\"llm-as-judge::base\"],\n",
    "        benchmark_config={\n",
    "            \"eval_candidate\": {\n",
    "                \"type\": \"model\",\n",
    "                \"model\": model_id,\n",
    "                \"sampling_params\": {\n",
    "                    \"strategy\": {\"type\": \"greedy\"},  # Deterministic generation\n",
    "                    \"max_tokens\": 512,               # Allow detailed answers\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # === Analyze Dataset Results ===\n",
    "    print(\"\\nğŸ“Š SimpleQA Evaluation Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, gen in enumerate(response.generations):\n",
    "        score = response.scores['llm-as-judge::base'].score_rows[i]\n",
    "        \n",
    "        print(f\"\\n{i+1}. Question: {eval_rows.data[i]['input_query']}\")\n",
    "        print(f\"   Expected: {eval_rows.data[i]['expected_answer']}\")\n",
    "        print(f\"   Generated: {gen['generated_answer']}\")\n",
    "        print(f\"   Judge Score: {score['score']}\")\n",
    "        print(f\"   Judge Feedback: {score.get('judge_feedback', 'No feedback')}\")\n",
    "        \n",
    "        # Categorize performance\n",
    "        if score['score'] in ['A', 'B', 'C', 'E']:\n",
    "            print(\"   ğŸ“ˆ Result: âœ… CORRECT/ACCEPTABLE\")\n",
    "        else:\n",
    "            print(\"   ğŸ“‰ Result: âŒ INCORRECT/INADEQUATE\")\n",
    "\n",
    "    # === Performance Summary ===\n",
    "    correct_count = sum(1 for score in response.scores['llm-as-judge::base'].score_rows \n",
    "                       if score['score'] in ['A', 'B', 'C', 'E'])\n",
    "    total_count = len(response.scores['llm-as-judge::base'].score_rows)\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "\n",
    "    print(f\"\\nğŸ¯ Dataset Performance Summary:\")\n",
    "    print(f\"   Correct: {correct_count}/{total_count} ({accuracy:.1%})\")\n",
    "    print(f\"   Model: {model_id}\")\n",
    "    print(f\"   Dataset: SimpleQA (factual knowledge)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Dataset evaluation failed: {e}\")\n",
    "    print(\"ğŸ’¡ This might be due to dataset availability or network connectivity issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 4: Academic Benchmark (MMLU-style) Evaluation\n",
    "\n",
    "Let's test on **standardized academic benchmarks** - the gold standard for comparing model capabilities across institutions and research papers.\n",
    "\n",
    "**MMLU (Massive Multitask Language Understanding):**\n",
    "- **Format**: Multiple choice questions (A, B, C, D)\n",
    "- **Subjects**: 57 academic domains (math, history, science, law, etc.)\n",
    "- **Difficulty**: High school to graduate level knowledge\n",
    "- **Scoring**: Binary correct/incorrect based on letter choice\n",
    "\n",
    "**Key Differences from Previous Methods:**\n",
    "- **Structured Output**: Must extract specific letter choice (A/B/C/D)\n",
    "- **System Prompts**: Use academic expert personas for better performance\n",
    "- **Regex Parsing**: Extract final answer from potentially verbose explanations\n",
    "- **Standardized**: Results comparable to published research\n",
    "\n",
    "**Why Academic Benchmarks Matter:**\n",
    "1. **Research Comparability**: Your results can be compared to published papers\n",
    "2. **Objective Metrics**: Clear pass/fail criteria with no ambiguity\n",
    "3. **Comprehensive Coverage**: Tests reasoning across multiple domains\n",
    "4. **Industry Standard**: Used by major AI companies for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u5t8jxp4dva",
   "metadata": {},
   "outputs": [],
   "source": "# === Step 4: Financial Knowledge Multiple Choice Evaluation ===\n\n# === Sample Financial Knowledge Questions ===\n# Basic financial concepts that don't require current data\nmmlu_sample_rows = [\n    {\n        \"input_query\": \"What does IPO stand for?\\nA) Initial Public Offering\\nB) International Private Organization\\nC) Investment Portfolio Option\\nD) Internal Profit Operation\",\n        \"expected_answer\": \"A\",\n        \"chat_completion_input\": '[{\"role\": \"user\", \"content\": \"What does IPO stand for?\\\\nA) Initial Public Offering\\\\nB) International Private Organization\\\\nC) Investment Portfolio Option\\\\nD) Internal Profit Operation\"}]'\n    },\n    {\n        \"input_query\": \"What does ROI stand for?\\nA) Rate of Interest\\nB) Return on Investment\\nC) Risk of Inflation\\nD) Revenue over Income\",\n        \"expected_answer\": \"B\",\n        \"chat_completion_input\": '[{\"role\": \"user\", \"content\": \"What does ROI stand for?\\\\nA) Rate of Interest\\\\nB) Return on Investment\\\\nC) Risk of Inflation\\\\nD) Revenue over Income\"}]'\n    },\n    {\n        \"input_query\": \"Who is known as the 'Oracle of Omaha'?\\nA) Bill Gates\\nB) Steve Jobs\\nC) Warren Buffett\\nD) Jeff Bezos\",\n        \"expected_answer\": \"C\",\n        \"chat_completion_input\": '[{\"role\": \"user\", \"content\": \"Who is known as the \\'Oracle of Omaha\\'?\\\\nA) Bill Gates\\\\nB) Steve Jobs\\\\nC) Warren Buffett\\\\nD) Jeff Bezos\"}]'\n    }\n]\n\nprint(\"ğŸ“ Financial Knowledge Multiple Choice Evaluation\")\nprint(\"ğŸ“š Testing basic financial concepts...\")\n\nfor i, row in enumerate(mmlu_sample_rows):\n    print(f\"\\n{i+1}. {row['input_query']}\")\n    print(f\"   Correct Answer: {row['expected_answer']}\")\n\n# === Create Financial Expert System Message ===\nsystem_message = {\n    \"role\": \"system\", \n    \"content\": \"You are a financial expert. Answer each multiple choice question by first reasoning about the correct answer, then providing your answer in the format 'Answer: X' where X is one of A, B, C, D.\",\n}\n\nprint(f\"\\nğŸ§ª Running financial knowledge evaluation with {model_id}...\")\n\ntry:\n    # === Register Financial Benchmark ===\n    client.benchmarks.register(\n        benchmark_id=\"meta-reference::financial-sample\",\n        dataset_id=\"financial-sample\", \n        scoring_functions=[],  # We'll use regex parser for multiple choice\n    )\n\n    # === Evaluate with Regex Parser for Multiple Choice ===\n    # This extracts the letter choice (A/B/C/D) from the model's response\n    response = client.eval.evaluate_rows(\n        benchmark_id=\"meta-reference::financial-sample\",\n        input_rows=mmlu_sample_rows,\n        scoring_functions=[\"basic::regex_parser_multiple_choice_answer\"],\n        benchmark_config={\n            \"eval_candidate\": {\n                \"type\": \"model\",\n                \"model\": model_id,\n                \"sampling_params\": {\n                    \"strategy\": {\n                        \"type\": \"top_p\",\n                        \"temperature\": 0.1,  # Low temperature for consistent answers\n                        \"top_p\": 0.95,\n                    },\n                    \"max_tokens\": 512,\n                },\n                \"system_message\": system_message,  # Use our financial expert prompt\n            },\n        },\n    )\n\n    # === Analyze Financial Results ===\n    print(\"\\nğŸ“Š Financial Knowledge Evaluation Results:\")\n    print(\"=\" * 70)\n    \n    for i, gen in enumerate(response.generations):\n        score = response.scores['basic::regex_parser_multiple_choice_answer'].score_rows[i]\n        \n        question_short = mmlu_sample_rows[i]['input_query'].split('?')[0] + \"?\"\n        print(f\"\\n{i+1}. Question: {question_short}\")\n        print(f\"   Expected: {mmlu_sample_rows[i]['expected_answer']}\")\n        print(f\"   Generated Response: {gen['generated_answer']}\")\n        print(f\"   Extracted Answer: {score.get('parsed_answer', 'Failed to parse')}\")\n        print(f\"   Score: {'âœ… CORRECT' if score['score'] == 1.0 else 'âŒ INCORRECT'} ({score['score']})\")\n\n    # === Calculate Financial Accuracy ===\n    results = response.scores['basic::regex_parser_multiple_choice_answer']\n    if 'accuracy' in results.aggregated_results:\n        accuracy = results.aggregated_results['accuracy']['accuracy']\n        correct = results.aggregated_results['accuracy']['num_correct']\n        total = results.aggregated_results['accuracy']['num_total']\n        \n        print(f\"\\nğŸ¯ Financial Knowledge Performance Summary:\")\n        print(f\"   Accuracy: {accuracy:.1%} ({correct}/{total})\")\n        print(f\"   Model: {model_id}\")\n        print(f\"   Format: Multiple Choice (A/B/C/D)\")\n        print(f\"   Topics: Basic Financial Concepts\")\n        \n        # Provide context for the results\n        if accuracy >= 0.8:\n            print(f\"   ğŸŒŸ Excellent performance on basic financial knowledge!\")\n        elif accuracy >= 0.6:\n            print(f\"   ğŸ“ˆ Good performance with room for improvement.\")\n        else:\n            print(f\"   ğŸ“‰ Consider improving financial knowledge training.\")\n    else:\n        print(\"   âš ï¸  Could not calculate aggregated accuracy metrics\")\n\nexcept Exception as e:\n    print(f\"âŒ Financial evaluation failed: {e}\")\n    print(\"ğŸ’¡ This might be due to parsing issues or model response formatting problems\")"
  },
  {
   "cell_type": "markdown",
   "id": "ujaam5dzna",
   "metadata": {},
   "source": [
    "## ğŸ‰ Comprehensive Evaluation Complete!\n",
    "\n",
    "**What you accomplished:**\n",
    "- **ğŸ¯ Basic Evaluation**: Implemented exact string matching with `subset_of` for fast, precise testing\n",
    "- **ğŸ¤– Semantic Evaluation**: Used `llm_as_judge` for flexible, context-aware evaluation with explanations\n",
    "- **ğŸ“š Dataset Testing**: Evaluated on real-world SimpleQA dataset for unbiased performance measurement\n",
    "- **ğŸ“ Academic Benchmarks**: Tested structured multiple-choice questions with regex parsing for standardized comparison\n",
    "- **ğŸ“Š End-to-End Pipeline**: Built complete evaluation workflows from question to performance metrics\n",
    "\n",
    "**Key Technical Insights:**\n",
    "- **Exact vs Semantic**: `subset_of` is fast but rigid; `llm_as_judge` is flexible but requires more resources\n",
    "- **Dataset Diversity**: Real datasets reveal different failure modes than handcrafted examples  \n",
    "- **Structured Evaluation**: Academic benchmarks provide standardized, comparable performance metrics\n",
    "- **Error Analysis**: Each method reveals different aspects of model capabilities and limitations\n",
    "\n",
    "**Evaluation Method Comparison:**\n",
    "| Method | Speed | Flexibility | Use Case | Cost |\n",
    "|--------|-------|-------------|----------|------|\n",
    "| `subset_of` | âš¡ Fast | ğŸ”’ Rigid | Quick sanity checks | ğŸ’° Free |\n",
    "| `llm_as_judge` | ğŸŒ Slower | ğŸ¤¸ Flexible | Nuanced evaluation | ğŸ’°ğŸ’° LLM calls |\n",
    "| Multiple Choice + Regex | âš¡ Fast | ğŸ“ Structured | Academic benchmarks | ğŸ’° Model calls only |\n",
    "\n",
    "**Production Best Practices:**\n",
    "1. **Multi-Method Approach**: Use all three methods together for comprehensive coverage\n",
    "2. **Judge Model Selection**: Use stronger models (Llama 3.3 70B, 405B) as judges, not the model being evaluated\n",
    "3. **Dataset Rotation**: Regularly test on new, unseen datasets to prevent overfitting\n",
    "4. **Error Analysis**: Investigate failure patterns to guide model improvements\n",
    "5. **Benchmark Tracking**: Monitor performance trends over time and model versions\n",
    "\n",
    "**Advanced Evaluation Patterns to Explore:**\n",
    "- **Multi-turn Conversations**: Evaluate dialogue consistency and context retention\n",
    "- **Tool Use Evaluation**: Test agent capabilities with external tools and APIs\n",
    "- **Safety & Alignment**: Evaluate harmful content detection and refusal behaviors\n",
    "- **Domain-specific Benchmarks**: Create custom evaluation datasets for your specific use case\n",
    "- **Human Evaluation**: Combine automated metrics with human judgment for complex tasks\n",
    "\n",
    "**Next Steps:**\n",
    "- Scale up evaluation with larger datasets (100s-1000s of examples)\n",
    "- Implement automated evaluation pipelines for continuous testing\n",
    "- Create custom evaluation datasets for your specific domain or use case\n",
    "- Set up performance monitoring and alerting for production model deployment\n",
    "\n",
    "Your evaluation framework is now production-ready - the foundation for deploying AI systems with confidence! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}