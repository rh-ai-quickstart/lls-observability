{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": "# üåê LangGraph Agents with Llama Stack: Bring Your Own Agentic Framework\n\nThis notebook demonstrates **agentic framework integration** - how to use any agentic framework (LangGraph, AutoGen, CrewAI) with Llama Stack's OpenAI-compatible APIs.\n\n**What is LangGraph?**\nLangGraph is a state-based agent framework that transforms AI applications into sophisticated multi-step reasoning systems:\n- **State Management**: Track conversation history and intermediate reasoning steps\n- **Graph-Based Flows**: Define complex agent workflows with conditional logic and loops\n- **Tool Integration**: Seamlessly bind external tools for enhanced capabilities\n- **Flexible Architecture**: Build everything from simple chatbots to complex multi-agent systems\n\n**Why LangGraph + Llama Stack?**\nInstead of being locked into a single provider's ecosystem, this combination gives you:\n- **Framework Freedom**: Use your preferred agentic framework without vendor lock-in\n- **OpenAI Compatibility**: Leverage existing LangChain/LangGraph code with minimal changes\n- **Tool Ecosystem**: Access MCP (Model Context Protocol) tools for weather, web search, and more\n- **Production Ready**: Deploy on your infrastructure with full observability and control\n\n**The Integration Magic:**\nLlama Stack's OpenAI-compatible endpoint (`/v1`) allows existing OpenAI-based frameworks to work seamlessly with locally deployed models and tools.\n\nLet's build intelligent agents that combine the best of both worlds! üöÄ"
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": "## üèóÔ∏è LangGraph + Llama Stack Architecture\n\nThe integration creates **three key layers** that work together to enable sophisticated agentic capabilities:\n\n### 1. üß† LangGraph Layer (The Agent Brain)\nThis is where intelligent agent behavior is defined:\n- **StateGraph**: Manages conversation state and agent memory across interactions\n- **Nodes & Edges**: Define agent reasoning steps and decision flow\n- **Message Handling**: Tracks conversation history and context\n- **Conditional Logic**: Enables complex multi-step reasoning workflows\n\n### 2. üîó OpenAI Compatibility Layer (The Translation)\nThis bridges LangGraph to Llama Stack seamlessly:\n- **ChatOpenAI Client**: Standard LangChain interface pointing to Llama Stack\n- **OpenAI-Compatible Endpoint**: Llama Stack's `/v1/openai/v1` endpoint\n- **Tool Binding**: Attach MCP tools to LLM for enhanced capabilities\n- **Response Handling**: Process streaming and non-streaming responses\n\n### 3. ü¶ô Llama Stack Layer (The Infrastructure)\nThis provides the AI model and tool runtime:\n- **Model Inference**: vLLM-powered Llama 3.2 3B for fast, local inference\n- **MCP Tools**: Weather, web search, and custom tool integrations\n- **Observability**: Comprehensive telemetry and monitoring\n- **Production Features**: Safety filters, rate limiting, and error handling\n\n### üîÑ Data Flow Architecture\n\n```\nUser Question ‚Üí LangGraph StateGraph ‚Üí ChatOpenAI Client ‚Üí Llama Stack OpenAI Endpoint\n                     ‚Üì                        ‚Üì                      ‚Üì\n               State Management          Tool Binding          Model Inference\n                     ‚Üì                        ‚Üì                      ‚Üì  \n               Agent Reasoning ‚Üê Tool Calls ‚Üê MCP Tools ‚Üê Tool Runtime\n                     ‚Üì\n               Final Response\n```\n\n**The Power**: LangGraph provides sophisticated agent orchestration while Llama Stack handles the heavy lifting of model inference and tool execution.\n\n## üì¶ Install Required Packages\n\nInstall the LangGraph and integration dependencies:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q langgraph==0.6.7 langchain-openai==0.3.32 langchain-core==0.3.75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# Core imports for LangGraph integration\nimport os\nimport sys\nimport json\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\n# LangGraph imports for agent creation\nfrom langgraph.graph import StateGraph, END, START\nfrom langgraph.graph.message import add_messages\n\n# LangChain imports for OpenAI compatibility\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain_core.tools import tool\n\n# Additional utilities for development\nfrom pprint import pprint"
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": "## üîó Connect LangGraph to Llama Stack\n\nConnect LangGraph to Llama Stack's OpenAI-compatible endpoint. This creates a seamless bridge that allows LangGraph to use Llama Stack as its inference backend while maintaining full compatibility with existing LangChain code.\n\n**Key Integration Points:**\n- **OpenAI-Compatible Endpoint**: Use Llama Stack's `/v1/openai/v1` endpoint for seamless integration\n- **Model Configuration**: Point to deployed Llama 3.2 3B model for fast inference\n- **Tool Binding**: Prepare for MCP weather tool integration\n- **State Management**: Set up LangGraph's conversation state handling"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [],
   "source": "# === LangGraph + Llama Stack Configuration ===\nprint(\"üåê Configuring LangGraph with Llama Stack Integration\")\n\n# === Endpoint Configuration ===\n# Use Llama Stack's OpenAI-compatible endpoint for seamless LangChain integration\nLLAMA_STACK_OPENAI_ENDPOINT = \"http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1\"\nINFERENCE_MODEL = \"llama3-2-3b\"  # Model deployed in the cluster\nAPI_KEY = \"not-applicable\"       # Not needed for local deployment\n\nprint(f\"üìç Llama Stack OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\nprint(f\"ü§ñ Inference Model: {INFERENCE_MODEL}\")\n\n# === Create ChatOpenAI Client ===\n# This creates a standard LangChain client that talks to Llama Stack\nllm = ChatOpenAI(\n    model=INFERENCE_MODEL,\n    openai_api_key=API_KEY,\n    openai_api_base=LLAMA_STACK_OPENAI_ENDPOINT,\n    temperature=0.1,  # Slightly creative but mostly deterministic\n    max_tokens=512,   # Reasonable response length\n)\n\nprint(\"‚úÖ ChatOpenAI client configured for Llama Stack\")\n\n# === Test Basic Connectivity ===\nprint(\"\\nüß™ Testing basic LangGraph-Llama Stack connectivity...\")\n\ntry:\n    # Simple connectivity test\n    response = llm.invoke(\"Hello! Please respond with 'Connection successful' if you can hear me.\")\n    print(f\"üì§ Test Query: Hello connectivity test\")\n    print(f\"üì• Response: {response.content}\")\n    print(\"‚úÖ Connection successful!\")\nexcept Exception as e:\n    print(f\"‚ùå Connection failed: {e}\")\n    print(\"üí° Make sure Llama Stack service is running and accessible\")\n    sys.exit(1)\n\nprint(f\"\\nüéØ LangGraph is now ready to use Llama Stack for inference!\")"
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": "## ü§ñ Step 1: Create a Basic LangGraph Agent\n\nNow let's build our first LangGraph agent that uses Llama Stack for inference. This demonstrates the core pattern for creating stateful, conversational agents.\n\n**LangGraph Fundamentals:**\n1. **State Definition**: Define what information the agent tracks across interactions\n2. **Node Creation**: Create functions that process messages and update state\n3. **Graph Building**: Connect nodes with edges to define conversation flow\n4. **Compilation**: Compile the graph into an executable agent\n\n**Key Benefits of LangGraph:**\n- **Memory**: Automatically tracks conversation history across interactions\n- **State Persistence**: Maintains context and intermediate results\n- **Flexible Flow**: Define complex multi-step reasoning patterns\n- **Tool Integration**: Easily bind external tools for enhanced capabilities"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# === STEP 1: Define LangGraph State ===\n# This defines what information our agent tracks across the conversation\nclass ConversationState(TypedDict):\n    \"\"\"\n    State schema for our conversational agent.\n    \n    messages: List of conversation messages with automatic deduplication\n    \"\"\"\n    messages: Annotated[list, add_messages]  # add_messages handles message deduplication\n\nprint(\"üìã Defined ConversationState with message tracking\")\n\n# === STEP 2: Create Agent Node ===\n# This is the core function that processes messages using Llama Stack\ndef chatbot_node(state: ConversationState):\n    \"\"\"\n    Core agent function that processes conversation state.\n    \n    Takes the current state, calls Llama Stack for inference,\n    and returns the updated state with the LLM response.\n    \"\"\"\n    # Extract messages from state\n    messages = state[\"messages\"]\n    \n    # Call Llama Stack via ChatOpenAI client\n    response = llm.invoke(messages)\n    \n    # Return updated state with new message\n    # LangGraph automatically merges this with existing state\n    return {\"messages\": [response]}\n\nprint(\"üß† Created chatbot_node for LLM inference\")\n\n# === STEP 3: Build LangGraph StateGraph ===\n# This defines the agent's conversation flow\nprint(\"\\nüèóÔ∏è Building LangGraph agent...\")\n\n# Create the graph builder\ngraph_builder = StateGraph(ConversationState)\n\n# Add our chatbot node\ngraph_builder.add_node(\"chatbot\", chatbot_node)\n\n# Define the conversation flow\ngraph_builder.add_edge(START, \"chatbot\")  # Start ‚Üí chatbot\ngraph_builder.add_edge(\"chatbot\", END)    # chatbot ‚Üí End\n\n# Compile the graph into an executable agent\nagent = graph_builder.compile()\n\nprint(\"‚úÖ LangGraph agent created successfully!\")\nprint(\"üìä Agent Structure:\")\nprint(\"   START ‚Üí chatbot ‚Üí END\")\n\n# === STEP 4: Test the Basic Agent ===\nprint(\"\\nüß™ Testing basic LangGraph agent...\")\n\n# Create initial state with a test message\ninitial_state = {\n    \"messages\": [HumanMessage(content=\"Hi! What can you help me with?\")]\n}\n\n# Run the agent\nresult = agent.invoke(initial_state)\n\n# Display the conversation\nprint(\"\\nüí¨ Conversation Result:\")\nfor i, message in enumerate(result[\"messages\"]):\n    if isinstance(message, HumanMessage):\n        print(f\"üë§ Human: {message.content}\")\n    elif isinstance(message, AIMessage):\n        print(f\"ü§ñ Agent: {message.content}\")\n\nprint(\"\\n‚úÖ Basic LangGraph agent is working with Llama Stack!\")"
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": "## üå§Ô∏è Step 2: Add MCP Weather Tools Integration\n\nNow let's enhance our agent with **external tool capabilities** using MCP (Model Context Protocol) weather tools. This transforms our basic chatbot into a powerful agent that can take actions in the real world.\n\n**MCP (Model Context Protocol) Benefits:**\n- **Standardized Interface**: Universal protocol for connecting AI models to external tools\n- **Dynamic Tool Discovery**: Tools can be added/removed without code changes\n- **Type Safety**: Structured tool definitions with proper parameter validation\n- **Scalable**: Works with any number of tool providers and complex tool chains\n\n**Weather Tool Integration:**\n- **Real-time Data**: Get current weather conditions for any location\n- **Structured Queries**: Proper parameter handling for city names and locations\n- **Error Handling**: Graceful fallbacks when weather data is unavailable\n- **Tool Binding**: Seamless integration with LangGraph's tool calling mechanism\n\n**The Integration Flow:**\n1. **Tool Definition**: Define weather tools with proper schemas\n2. **LLM Binding**: Attach tools to our ChatOpenAI client  \n3. **Agent Enhancement**: Update our agent to handle tool calls\n4. **Testing**: Demonstrate weather queries with real-time data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# === STEP 1: Configure MCP Weather Tools ===\nprint(\"üå§Ô∏è Setting up MCP weather tools integration...\")\n\n# Create a new LLM client with weather tools bound\n# This leverages Llama Stack's MCP tool runtime for weather data\nllm_with_tools = llm.bind_tools([\n    {\n        \"type\": \"mcp\",                                          # Use MCP protocol\n        \"server_label\": \"weather\",                              # Tool group identifier\n        \"server_url\": \"http://mcp-weather.llama-serve.svc.cluster.local:80/sse\",    # MCP weather service endpoint\n        \"require_approval\": \"never\",                            # Auto-approve tool calls for demo\n    }\n])\n\nprint(\"‚úÖ MCP weather tools configured\")\nprint(\"üì° Weather Service: http://mcp-weather.llama-serve.svc.cluster.local:80/sse\")\n\n# === STEP 2: Enhanced Agent Node with Tool Support ===\ndef enhanced_chatbot_node(state: ConversationState):\n    \"\"\"\n    Enhanced agent function that supports tool calling.\n    \n    This version can:\n    1. Process normal conversation messages\n    2. Make tool calls when appropriate\n    3. Handle tool responses and integrate them into the conversation\n    \"\"\"\n    messages = state[\"messages\"]\n    \n    # Call Llama Stack with tool-enabled LLM\n    response = llm_with_tools.invoke(messages)\n    \n    return {\"messages\": [response]}\n\nprint(\"üõ†Ô∏è Created enhanced_chatbot_node with tool support\")\n\n# === STEP 3: Build Enhanced LangGraph Agent ===\nprint(\"\\nüèóÔ∏è Building enhanced LangGraph agent with weather tools...\")\n\n# Create new graph builder for enhanced agent\nenhanced_graph_builder = StateGraph(ConversationState)\n\n# Add the enhanced chatbot node\nenhanced_graph_builder.add_node(\"enhanced_chatbot\", enhanced_chatbot_node)\n\n# Define the conversation flow (same as before, but with tool capabilities)\nenhanced_graph_builder.add_edge(START, \"enhanced_chatbot\")\nenhanced_graph_builder.add_edge(\"enhanced_chatbot\", END)\n\n# Compile the enhanced agent\nenhanced_agent = enhanced_graph_builder.compile()\n\nprint(\"‚úÖ Enhanced LangGraph agent created successfully!\")\nprint(\"üîß New Capabilities: Weather queries via MCP tools\")\n\n# === STEP 4: Test Weather Tool Integration ===\nprint(\"\\nüß™ Testing weather tool integration...\")\n\n# Test with a weather query\nweather_query = {\n    \"messages\": [HumanMessage(content=\"What's the weather like in Seattle today?\")]\n}\n\nprint(\"üì§ Query: What's the weather like in Seattle today?\")\n\ntry:\n    # Run the enhanced agent with weather query\n    weather_result = enhanced_agent.invoke(weather_query)\n    \n    # Display the conversation with tool usage\n    print(\"\\nüå¶Ô∏è Weather Query Result:\")\n    for message in weather_result[\"messages\"]:\n        if isinstance(message, HumanMessage):\n            print(f\"üë§ Human: {message.content}\")\n        elif isinstance(message, AIMessage):\n            print(f\"ü§ñ Agent: {message.content}\")\n            \n            # Check if the agent made any tool calls\n            if hasattr(message, 'tool_calls') and message.tool_calls:\n                print(f\"üîß Tool Calls Made: {len(message.tool_calls)}\")\n                for tool_call in message.tool_calls:\n                    print(f\"   üì° Called: {tool_call.get('name', 'unknown tool')}\")\n\n    print(\"\\n‚úÖ Weather tool integration successful!\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Weather tool test failed: {e}\")\n    print(\"üí° Make sure the MCP weather service is running and accessible\")\n    \n    # Fallback to basic agent for demonstration\n    print(\"\\nüîÑ Falling back to basic conversation...\")\n    fallback_result = agent.invoke({\n        \"messages\": [HumanMessage(content=\"Tell me about the weather in general.\")]\n    })\n    \n    print(\"üåà Fallback Response:\")\n    for message in fallback_result[\"messages\"]:\n        if isinstance(message, AIMessage):\n            print(f\"ü§ñ Agent: {message.content}\")\n\nprint(f\"\\nüéØ LangGraph agent now has weather tool capabilities!\")"
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": "## üöÄ Step 3: Advanced Agent Patterns & Multi-Turn Conversations\n\nNow let's explore **advanced LangGraph patterns** that demonstrate the full power of stateful, multi-turn conversations with tool integration.\n\n**Advanced Patterns We'll Implement:**\n- **Multi-Turn Conversations**: Maintain context across multiple interactions\n- **Conditional Logic**: Agent decides when to use tools vs. conversation\n- **Error Handling**: Graceful fallbacks when tools fail or are unavailable\n- **Context Persistence**: Remember previous tool calls and results\n- **Interactive Workflows**: Guide users through multi-step processes\n\n**Why This Matters:**\n1. **Real-world Readiness**: Production agents need to handle complex interactions\n2. **User Experience**: Smooth conversations feel more natural and helpful\n3. **Reliability**: Robust error handling prevents agent failures\n4. **Scalability**: Patterns work across different tool types and use cases\n\n**Conversation Flow Example:**\n```\nUser: \"What's the weather in Seattle?\"\nAgent: [Calls weather tool] ‚Üí \"It's 72¬∞F and sunny in Seattle\"\n\nUser: \"How about Portland?\"  \nAgent: [Remembers context, calls weather tool] ‚Üí \"Portland is 68¬∞F with light rain\"\n\nUser: \"Which city should I visit?\"\nAgent: [Uses previous weather data] ‚Üí \"Seattle has better weather today!\"\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# === STEP 1: Advanced Multi-Turn Conversation Demo ===\nprint(\"üöÄ Demonstrating advanced multi-turn conversation patterns...\")\n\n# Start with an empty conversation state\nconversation_state = {\"messages\": []}\n\ndef chat_with_agent(user_input, state):\n    \"\"\"\n    Helper function to simulate multi-turn conversations.\n    \n    Adds user input to state, runs agent, and returns updated state.\n    \"\"\"\n    # Add user message to conversation\n    state[\"messages\"].append(HumanMessage(content=user_input))\n    \n    # Run the enhanced agent\n    result = enhanced_agent.invoke(state)\n    \n    # Return the updated state for next turn\n    return result\n\n# === STEP 2: Multi-Turn Weather Conversation ===\nprint(\"\\nüí¨ Multi-Turn Weather Conversation Demo:\")\nprint(\"=\" * 50)\n\n# Turn 1: Ask about Seattle weather\nprint(\"üó£Ô∏è Turn 1: Initial weather query\")\nconversation_state = chat_with_agent(\n    \"What's the weather like in Seattle?\", \n    conversation_state\n)\n\n# Display latest response\nlatest_response = conversation_state[\"messages\"][-1]\nif isinstance(latest_response, AIMessage):\n    print(f\"ü§ñ Agent: {latest_response.content}\")\n\n# Turn 2: Ask about another city (tests context retention)\nprint(f\"\\nüó£Ô∏è Turn 2: Follow-up weather query\")\nconversation_state = chat_with_agent(\n    \"How about the weather in Portland?\", \n    conversation_state\n)\n\nlatest_response = conversation_state[\"messages\"][-1]\nif isinstance(latest_response, AIMessage):\n    print(f\"ü§ñ Agent: {latest_response.content}\")\n\n# Turn 3: Ask for comparison (tests reasoning with previous tool results)\nprint(f\"\\nüó£Ô∏è Turn 3: Comparison based on previous queries\")\nconversation_state = chat_with_agent(\n    \"Based on the weather, which city would be better to visit today?\", \n    conversation_state\n)\n\nlatest_response = conversation_state[\"messages\"][-1]\nif isinstance(latest_response, AIMessage):\n    print(f\"ü§ñ Agent: {latest_response.content}\")\n\n# === STEP 3: Display Full Conversation History ===\nprint(f\"\\nüìö Complete Conversation History:\")\nprint(\"=\" * 50)\n\nfor i, message in enumerate(conversation_state[\"messages\"]):\n    if isinstance(message, HumanMessage):\n        print(f\"üë§ Human ({i+1}): {message.content}\")\n    elif isinstance(message, AIMessage):\n        print(f\"ü§ñ Agent ({i+1}): {message.content}\")\n        \n        # Show tool usage if any\n        if hasattr(message, 'tool_calls') and message.tool_calls:\n            print(f\"   üîß Used {len(message.tool_calls)} tool(s)\")\n    print()\n\n# === STEP 4: Demonstrate Error Handling ===\nprint(\"üõ°Ô∏è Testing Error Handling & Fallback Patterns:\")\nprint(\"=\" * 50)\n\n# Test with a query that might fail or require fallback\nerror_test_state = {\"messages\": []}\n\ntry:\n    error_test_state = chat_with_agent(\n        \"What's the weather on Mars? If you can't find that, just tell me about space weather in general.\", \n        error_test_state\n    )\n    \n    latest_response = error_test_state[\"messages\"][-1]\n    if isinstance(latest_response, AIMessage):\n        print(f\"ü§ñ Graceful Response: {latest_response.content}\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error occurred: {e}\")\n    print(\"üí° In production, implement retry logic and user-friendly error messages\")\n\n# === STEP 5: Performance Insights ===\nprint(f\"\\nüìä Conversation Analysis:\")\nprint(f\"   üí¨ Total messages: {len(conversation_state['messages'])}\")\nprint(f\"   üîÑ Conversation turns: {len([m for m in conversation_state['messages'] if isinstance(m, HumanMessage)])}\")\nprint(f\"   ü§ñ Agent responses: {len([m for m in conversation_state['messages'] if isinstance(m, AIMessage)])}\")\n\n# Count tool usage across conversation\ntool_usage_count = 0\nfor message in conversation_state[\"messages\"]:\n    if isinstance(message, AIMessage) and hasattr(message, 'tool_calls') and message.tool_calls:\n        tool_usage_count += len(message.tool_calls)\n\nprint(f\"   üîß Tools called: {tool_usage_count}\")\nprint(f\"   üìà Context retention: {'‚úÖ Working' if len(conversation_state['messages']) > 2 else '‚ùå Needs improvement'}\")\n\nprint(f\"\\n‚úÖ Advanced conversation patterns demonstrated successfully!\")"
  },
  {
   "cell_type": "markdown",
   "id": "5bb5b323",
   "metadata": {},
   "source": "## üéâ LangGraph + Llama Stack Integration Complete!\n\n**What you accomplished:**\n- **üåê Framework Integration**: Successfully connected LangGraph to Llama Stack's OpenAI-compatible endpoint\n- **ü§ñ Basic Agent**: Created stateful conversational agents with message history and context management\n- **üõ†Ô∏è Tool Integration**: Enhanced agents with MCP weather tools for real-world capabilities\n- **üöÄ Advanced Patterns**: Implemented multi-turn conversations with context retention and error handling\n- **üìä Production Patterns**: Demonstrated conversation analysis and performance monitoring\n\n**Key Technical Insights:**\n- **OpenAI Compatibility**: Seamless integration requires no LangGraph code changes when switching providers\n- **State Management**: LangGraph automatically handles conversation history and state persistence\n- **Tool Binding**: MCP tools integrate naturally with LangChain's tool calling mechanism\n- **Error Resilience**: Robust agents gracefully handle tool failures and unexpected inputs\n\n**Architecture Benefits:**\n| Traditional Approach | LangGraph + Llama Stack |\n|---------------------|-------------------------|\n| ‚ùå Vendor lock-in | ‚úÖ Framework freedom |\n| ‚ùå Simple request/response | ‚úÖ Stateful conversations |\n| ‚ùå Manual tool orchestration | ‚úÖ Automated tool calling |\n| ‚ùå Limited context | ‚úÖ Persistent memory |\n| ‚ùå Cloud dependency | ‚úÖ On-premise deployment |\n\n**Production Best Practices:**\n1. **Error Handling**: Always implement fallback patterns for tool failures\n2. **State Management**: Use LangGraph's checkpointing for conversation persistence\n3. **Tool Security**: Validate tool inputs and implement approval workflows for sensitive operations\n4. **Performance**: Monitor token usage and response times across conversation turns\n5. **Observability**: Leverage Llama Stack's built-in telemetry for agent monitoring\n\n**Advanced Patterns to Explore:**\n- **Multi-Agent Systems**: Coordinate multiple specialized agents for complex tasks\n- **Conditional Flows**: Implement branching logic based on user input or context\n- **Custom Tools**: Create domain-specific tools using MCP protocol\n- **Async Operations**: Handle long-running tool calls with streaming responses\n- **RAG Integration**: Combine document retrieval with conversational agents\n\n**Real-World Applications:**\n- **Customer Support**: Context-aware agents that remember customer history\n- **Data Analysis**: Agents that can query databases and analyze results conversationally\n- **DevOps Automation**: Agents that can execute commands and explain system status\n- **Research Assistants**: Agents that can search, summarize, and reason about information\n\n**Next Steps:**\n- Explore other agentic frameworks (AutoGen, CrewAI) with the same Llama Stack backend\n- Build domain-specific tools using MCP for your use case\n- Implement production deployment with proper monitoring and scaling\n- Integrate with existing business systems and workflows\n\nYour LangGraph agents are now production-ready with the full power of Llama Stack's infrastructure! üöÄ"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}