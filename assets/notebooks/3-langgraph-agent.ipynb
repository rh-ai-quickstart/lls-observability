{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# üåê LangGraph Agents with Llama Stack: Bring Your Own Agentic Framework\n",
    "\n",
    "This notebook demonstrates **agentic framework integration** - how to use any agentic framework (LangGraph, AutoGen, CrewAI) with Llama Stack's OpenAI-compatible APIs.\n",
    "\n",
    "**What is LangGraph?**\n",
    "LangGraph is a state-based agent framework that transforms AI applications into sophisticated multi-step reasoning systems:\n",
    "- **State Management**: Track conversation history and intermediate reasoning steps\n",
    "- **Graph-Based Flows**: Define complex agent workflows with conditional logic and loops\n",
    "- **Tool Integration**: Seamlessly bind external tools for enhanced capabilities\n",
    "- **Flexible Architecture**: Build everything from simple chatbots to complex multi-agent systems\n",
    "\n",
    "**Why LangGraph + Llama Stack?**\n",
    "Instead of being locked into a single provider's ecosystem, this combination gives you:\n",
    "- **Framework Freedom**: Use your preferred agentic framework without vendor lock-in\n",
    "- **OpenAI Compatibility**: Leverage existing LangChain/LangGraph code with minimal changes\n",
    "- **Tool Ecosystem**: Access MCP (Model Context Protocol) tools for weather, web search, and more\n",
    "- **Production Ready**: Deploy on your infrastructure with full observability and control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## üèóÔ∏è LangGraph + Llama Stack Architecture\n",
    "\n",
    "The integration creates **three key layers** that work together to enable sophisticated agentic capabilities:\n",
    "\n",
    "### 1. üß† LangGraph Layer (The Agent Brain)\n",
    "This is where intelligent agent behavior is defined:\n",
    "- **StateGraph**: Manages conversation state and agent memory across interactions\n",
    "- **Nodes & Edges**: Define agent reasoning steps and decision flow\n",
    "- **Message Handling**: Tracks conversation history and context\n",
    "- **Conditional Logic**: Enables complex multi-step reasoning workflows\n",
    "\n",
    "### 2. üîó OpenAI Compatibility Layer (The Translation)\n",
    "This bridges LangGraph to Llama Stack seamlessly:\n",
    "- **ChatOpenAI Client**: Standard LangChain interface pointing to Llama Stack\n",
    "- **OpenAI-Compatible Endpoint**: Llama Stack's `/v1/openai/v1` endpoint\n",
    "- **Tool Binding**: Attach MCP tools to LLM for enhanced capabilities\n",
    "- **Response Handling**: Process streaming and non-streaming responses\n",
    "\n",
    "### 3. ü¶ô Llama Stack Layer (The AI API to run them all)\n",
    "This provides the AI model and tool runtime:\n",
    "- **Model Inference**: vLLM-powered Llama 3.2 3B for fast, local inference\n",
    "- **MCP Tools**: Weather, web search, and custom tool integrations\n",
    "- **Observability**: Comprehensive telemetry and monitoring\n",
    "- **Production Features**: Safety filters, rate limiting, and error handling\n",
    "\n",
    "**The Power**: LangGraph provides sophisticated agent orchestration while Llama Stack handles the heavy lifting of model inference and tool execution.\n",
    "\n",
    "## üì¶ Install Required Packages\n",
    "\n",
    "Install the LangGraph and integration dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332e6cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langgraph==0.6.7 langchain-openai==0.3.32 langchain-core==0.3.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Core imports for LangGraph integration\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# LangGraph imports for agent creation\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# LangChain imports for OpenAI compatibility\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Additional utilities for development\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "## üîó Connect LangGraph to Llama Stack\n",
    "\n",
    "Connect LangGraph to Llama Stack's OpenAI-compatible endpoint. This creates a seamless bridge that allows LangGraph to use Llama Stack as its inference backend while maintaining full compatibility with existing LangChain code.\n",
    "\n",
    "**Key Integration Points:**\n",
    "- **OpenAI-Compatible Endpoint**: Use Llama Stack's `/v1/openai/v1` endpoint for seamless integration\n",
    "- **Model Configuration**: Point to deployed Llama 3.2 3B model for fast inference\n",
    "- **Tool Binding**: Prepare for MCP weather tool integration\n",
    "- **State Management**: Set up LangGraph's conversation state handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Configuring LangGraph with Llama Stack Integration\n",
      "üìç Llama Stack OpenAI Endpoint: http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/openai/v1\n",
      "ü§ñ Inference Model: llama3-2-3b\n",
      "‚úÖ ChatOpenAI client configured for Llama Stack\n",
      "\n",
      "üß™ Testing basic LangGraph-Llama Stack connectivity...\n",
      "üì§ Test Query: Hello connectivity test\n",
      "üì• Response: [{'type': 'text', 'text': 'Connection successful', 'annotations': []}]\n",
      "‚úÖ Connection successful!\n",
      "\n",
      "üéØ LangGraph is now ready to use Llama Stack for inference!\n"
     ]
    }
   ],
   "source": [
    "# === LangGraph + Llama Stack Configuration ===\n",
    "print(\"üåê Configuring LangGraph with Llama Stack Integration\")\n",
    "\n",
    "# === Endpoint Configuration ===\n",
    "# Use Llama Stack's OpenAI-compatible endpoint for seamless LangChain integration\n",
    "LLAMA_STACK_OPENAI_ENDPOINT = \"http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/openai/v1\"\n",
    "INFERENCE_MODEL = \"llama3-2-3b\"  # Model deployed in the cluster\n",
    "API_KEY = \"fake\"       # Not needed for local deployment\n",
    "\n",
    "print(f\"üìç Llama Stack OpenAI Endpoint: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "print(f\"ü§ñ Inference Model: {INFERENCE_MODEL}\")\n",
    "\n",
    "# === Create ChatOpenAI Client ===\n",
    "# This creates a standard LangChain client that talks to Llama Stack\n",
    "llm = ChatOpenAI(\n",
    "    model=INFERENCE_MODEL,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base=LLAMA_STACK_OPENAI_ENDPOINT,\n",
    "    use_responses_api=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChatOpenAI client configured for Llama Stack\")\n",
    "\n",
    "# === Test Basic Connectivity ===\n",
    "print(\"\\nüß™ Testing basic LangGraph-Llama Stack connectivity...\")\n",
    "\n",
    "try:\n",
    "    # Simple connectivity test\n",
    "    response = llm.invoke(\"Hello! Please respond with 'Connection successful' if you can hear me.\")\n",
    "    print(f\"üì§ Test Query: Hello connectivity test\")\n",
    "    print(f\"üì• Response: {response.content}\")\n",
    "    print(\"‚úÖ Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    print(\"üí° Make sure Llama Stack service is running and accessible\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\nüéØ LangGraph is now ready to use Llama Stack for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "## ü§ñ Step 1: Create a Basic LangGraph Agent\n",
    "\n",
    "Now let's build our first LangGraph agent that uses Llama Stack for inference. This demonstrates the core pattern for creating stateful, conversational agents.\n",
    "\n",
    "**LangGraph Fundamentals:**\n",
    "1. **State Definition**: Define what information the agent tracks across interactions\n",
    "2. **Node Creation**: Create functions that process messages and update state\n",
    "3. **Graph Building**: Connect nodes with edges to define conversation flow\n",
    "4. **Compilation**: Compile the graph into an executable agent\n",
    "\n",
    "**Key Benefits of LangGraph:**\n",
    "- **Memory**: Automatically tracks conversation history across interactions\n",
    "- **State Persistence**: Maintains context and intermediate results\n",
    "- **Flexible Flow**: Define complex multi-step reasoning patterns\n",
    "- **Tool Integration**: Easily bind external tools for enhanced capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã LangGraph + Llama Stack Integration\n",
      "   LLAMA_STACK_URL: http://llama-stack-instance-service.llama-serve.svc.cluster.local:8321/v1/openai/v1\n",
      "   INFERENCE_MODEL: llama3-2-3b\n",
      "\n",
      "üß™ Testing basic connectivity:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connection successful\n",
      "\n",
      "üõ†Ô∏è Setting up MCP weather tools...\n",
      "‚úÖ MCP tools configured\n",
      "\n",
      "üèóÔ∏è Building LangGraph agent...\n",
      "‚úÖ LangGraph agent ready\n",
      "\n",
      "==================================================\n",
      "üöÄ Testing LangGraph Agent with MCP Tools\n",
      "==================================================\n",
      "Weather Response:\n",
      "What is the weather in Boston?\n",
      "Unfortunately, I don't have direct access to real-time weather information. However, I can suggest some alternative ways for you to find the current weather in Boston. You can check online weather websites such as weather.com or accuweather.com, or download a weather app on your smartphone to get the latest forecast.\n"
     ]
    }
   ],
   "source": [
    "print(\"üìã LangGraph + Llama Stack Integration\")\n",
    "print(f\"   LLAMA_STACK_URL: {LLAMA_STACK_OPENAI_ENDPOINT}\")\n",
    "print(f\"   INFERENCE_MODEL: {INFERENCE_MODEL}\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=INFERENCE_MODEL,\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base=LLAMA_STACK_OPENAI_ENDPOINT,\n",
    "    use_responses_api=True,\n",
    ")\n",
    "\n",
    "# Test connectivity\n",
    "print(\"\\nüß™ Testing basic connectivity:\")\n",
    "response = llm.invoke(\"Hello\")\n",
    "print(f\"‚úÖ Connection successful\")\n",
    "\n",
    "# Bind MCP weather tools\n",
    "print(\"\\nüõ†Ô∏è Setting up MCP weather tools...\")\n",
    "llm_with_tools = llm.bind_tools([\n",
    "    {\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"weather\",\n",
    "        \"server_url\": \"http://mcp-weather.llama-serve.svc.cluster.local:80/sse\",\n",
    "        \"require_approval\": \"never\",\n",
    "    },\n",
    "])\n",
    "print(\"‚úÖ MCP tools configured\")\n",
    "\n",
    "# Define LangGraph State and Agent\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "# Build LangGraph StateGraph\n",
    "print(\"\\nüèóÔ∏è Building LangGraph agent...\")\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "print(\"‚úÖ LangGraph agent ready\")\n",
    "\n",
    "# Test the integration\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Testing LangGraph Agent with MCP Tools\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "response = graph.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}]\n",
    "})\n",
    "\n",
    "print(\"Weather Response:\")\n",
    "for message in response['messages']:\n",
    "    if hasattr(message, 'content'):\n",
    "        if isinstance(message.content, list):\n",
    "            for content_block in message.content:\n",
    "                if content_block.get('type') == 'text':\n",
    "                    print(content_block.get('text', ''))\n",
    "        elif isinstance(message.content, str):\n",
    "            print(message.content)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
